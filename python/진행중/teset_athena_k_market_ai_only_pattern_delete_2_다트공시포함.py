# -*- coding: utf-8 -*-
"""
ğŸ“˜ teset_athena_k_market_ai_only_pattern_delete.py (v1.3 - ë‹¨ì¼ ì¢…ëª© í•„í„°ë§ ìµœì¢… í†µí•© ë²„ì „)
--------------------------------------------
âœ… í•œêµ­ ì£¼ì‹ ì‹œì¥ ë°ì´í„° ë¶„ì„ ë° ê¸°ìˆ ì  íŒ¨í„´ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸
Â  Â  - ê¸°ëŠ¥: ì¢…ëª© ë¶„ì„ í•„í„°ë§ (analyze ëª¨ë“œ), ì°¨íŠ¸ ì‹œê°í™” ë°ì´í„° ìƒì„± (chart ëª¨ë“œ)
Â  Â  - íŠ¹ì§•: DART APIë¥¼ í†µí•œ 'ë‹¨ì¼íŒë§¤Â·ê³µê¸‰ê³„ì•½ ì²´ê²°' ê³µì‹œ ì •ë³´ í†µí•© ë° **Corp Code ë§¤í•‘ ë¡œì§ ì¶”ê°€**
Â  Â  - ìˆ˜ì •: **--symbol ì¸ìë¥¼ í†µí•œ ë‹¨ì¼ ì¢…ëª© ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€**
"""

import os
import sys
import json
import time
import logging
import argparse
import traceback
import socket
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import glob

import pandas as pd
import numpy as np
from scipy.signal import find_peaks
import ta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# â­ DART API í˜¸ì¶œì„ ìœ„í•œ requests ë¼ì´ë¸ŒëŸ¬ë¦¬
# (ì£¼ì˜: ì‚¬ìš© ì „ 'pip install requests'ê°€ í•„ìš”í•©ë‹ˆë‹¤)
try:
    import requests
except ImportError:
    requests = None
    logging.warning("requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DART API ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (pip install requests í•„ìš”)")


# ==============================
# 1. ì´ˆê¸° ì•ˆì „ ê²€ì‚¬ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================

def safe_print_json(data, status_code=1):
    """í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ JSONì„ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    try:
        # CustomJsonEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ np íƒ€ì… ë° datetime ê°ì²´ ì²˜ë¦¬
        sys.stdout.write(json.dumps(data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
    except Exception as e:
        sys.stdout.write(json.dumps({"error": "JSON_SERIALIZATION_FAIL", "original_error": str(e)}, ensure_ascii=False) + "\n")
        
    sys.stdout.flush()
    if status_code != 0:
        sys.exit(status_code)

def check_internet_connection(host="8.8.8.8", port=53, timeout=3):
    """ê°„ë‹¨í•œ ì†Œì¼“ ì—°ê²°ì„ í†µí•´ ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        socket.setdefaulttimeout(timeout)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        s.close()
        return True
    except Exception:
        return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì¸í„°ë„· ì—°ê²° í™•ì¸
if not check_internet_connection():
    safe_print_json({"error": "CRITICAL_ERROR", "reason": "ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "mode": "initial_check"})

# ==============================
# 1.5. JSON Custom Encoder ì •ì˜
# ==============================
class CustomJsonEncoder(json.JSONEncoder):
    """NumPy íƒ€ì… ë° Pandas Timestampë¥¼ í‘œì¤€ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        if isinstance(obj, (np.floating, np.float64, np.float32)):
            if np.isnan(obj):
                return None
            return float(obj)
        if isinstance(obj, set):
            return list(obj)
        if isinstance(obj, (pd.Timestamp, datetime, np.datetime64)):
            return obj.strftime('%Y-%m-%d')
        return json.JSONEncoder.default(self, obj)


# ==============================
# 2. ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
# ==============================
# BASE_DIR ê²½ë¡œ ì„¤ì •ì„ í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ìœ„ì¹˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. (Path(__file__)ë¡œ ì‹œì‘)
BASE_DIR = Path(__file__).resolve().parent
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data" 
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json" 
CACHE_DIR = BASE_DIR / "cache" 
LOG_FILE = LOG_DIR / "stock_analyzer_ultimate.log"

# DART API ìƒìˆ˜ ì„¤ì •
DART_API_URL = "https://opendart.fss.or.kr/api/list.json"
DART_SEARCH_TERM = "ë‹¨ì¼íŒë§¤Â·ê³µê¸‰ê³„ì•½ ì²´ê²°" 

# DART ë§¤í•‘ ë¡œì§ì„ ìœ„í•œ ìƒìˆ˜/ì „ì—­ ë³€ìˆ˜ ì¶”ê°€
DART_CORP_MAP_FILE = BASE_DIR / "data" / "stock_list" / "dart_corp_codes.json"
DART_CORP_CODE_MAP = {} # Ticker-CorpCode ë§¤í•‘ ë”•ì…”ë„ˆë¦¬


# ==============================
# 3. í™˜ê²½ ì´ˆê¸°í™” ë° ìœ í‹¸ë¦¬í‹°
# ==============================

# DART Corp Code ë§¤í•‘ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_dart_corp_map():
    """dart_corp_codes.json íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì „ì—­ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    global DART_CORP_CODE_MAP
    if not DART_CORP_MAP_FILE.exists():
        logging.error(f"DART ë§¤í•‘ íŒŒì¼ ì—†ìŒ: {DART_CORP_MAP_FILE}")
        return
    try:
        with open(DART_CORP_MAP_FILE, "r", encoding="utf-8") as f:
            DART_CORP_CODE_MAP = json.load(f)
        logging.info(f"DART Corp Code ë§¤í•‘ ì •ë³´ {len(DART_CORP_CODE_MAP)}ê°œ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        logging.error(f"DART ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

def setup_env(log_level=logging.INFO):
    """í™˜ê²½ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•˜ê³  ë¡œê¹…ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    # DART ë§¤í•‘ ë¡œë”© í˜¸ì¶œ ì¶”ê°€
    load_dart_corp_map() 

    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8", mode='a'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def load_listing():
    """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ (stock_listing.json)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    default_item = [{"Code": "005930", "Name": "ì‚¼ì„±ì „ì"}]
    if not LISTING_FILE.exists():
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {LISTING_FILE}")
        return default_item
    try:
        with open(LISTING_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return default_item

def get_stock_name(symbol):
    """ì¢…ëª© ì½”ë“œë¡œ ì´ë¦„ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        items = load_listing()
        for item in items:
            code = item.get("Code") or item.get("code")
            if code == symbol: return item.get("Name") or item.get("name")
        return symbol
    except Exception: return symbol

# ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
def cleanup_old_cache(days=7):
    """ì§€ì •ëœ ê¸°ê°„(ì¼)ë³´ë‹¤ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    logging.info(f"ë§Œë£Œëœ ({days}ì¼ ì´ìƒ) ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œì‘.")
    
    cutoff_time = datetime.now() - timedelta(days=days)
    
    cache_files = CACHE_DIR.glob('*.json')
    
    deleted_count = 0
    for file_path in cache_files:
        try:
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if mod_time < cutoff_time:
                file_path.unlink()  
                deleted_count += 1
                logging.debug(f"ìºì‹œ íŒŒì¼ ì‚­ì œ: {file_path.name}")
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ {file_path.name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    logging.info(f"ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")


# ==============================
# 4. ê³ ê¸‰ íŠ¹ì§• ê³µí•™ ë° í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§
# ==============================

def calculate_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """ê³ ê¸‰ íŒ¨í„´ ì¸ì‹ì„ ìœ„í•´ ê¸°ìˆ ì  ì§€í‘œë¥¼ íŠ¹ì§•(Feature)ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."""
    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()
    df['MACD'] = ta.trend.MACD(close=df['Close'], fillna=False).macd()
    df['MACD_Signal'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_signal()
    df['MACD_Hist'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_diff() 

    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)
    df['BB_Width'] = bollinger.bollinger_wband()

    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()
    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50, fillna=False).sma_indicator()
    df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200, fillna=False).sma_indicator()

    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    df['TREND_CROSS'] = (df['SMA_50'] > df['SMA_200']).astype(int)

    feature_subset = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'SMA_200', 'Log_Return']
    df_with_features = df.copy().dropna(subset=feature_subset)
    return df_with_features

def add_market_regime_clustering(df_full: pd.DataFrame, n_clusters=4) -> pd.DataFrame:
    """K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì‹œì¥ êµ­ë©´(Market Regime)ì„ ì •ì˜í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤."""
    feature_cols = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'Log_Return'] 
    min_data_length = 200

    if len(df_full) < min_data_length or not all(col in df_full.columns for col in feature_cols):
        df_full['MarketRegime'] = -1 
        return df_full

    data = df_full[feature_cols].copy()

    if data.drop_duplicates().shape[0] < n_clusters:
        df_full['MarketRegime'] = -1 
        return df_full
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data) 

    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
        df_full['MarketRegime'] = kmeans.fit_predict(scaled_data) 
    except ValueError as e:
        df_full['MarketRegime'] = -1

    return df_full


# ==============================
# 5. ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§
# ==============================

def find_peaks_and_troughs(df, prominence_ratio=0.005, width=3):
    """ì£¼ìš” ë´‰ìš°ë¦¬(Peaks)ì™€ ê³¨ì§œê¸°(Troughs) ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤ (ìµœê·¼ 250ì¼ ê¸°ì¤€)."""
    recent_df = df.iloc[-250:].copy()
    if recent_df.empty: return np.array([]), np.array([])
    # Note: Use a fixed window for std to prevent instability if data changes often
    std_dev = recent_df['Close'].std() 
    prominence_val = std_dev * prominence_ratio 
    
    peaks, _ = find_peaks(recent_df['Close'], prominence=prominence_val, width=width)
    troughs, _ = find_peaks(-recent_df['Close'], prominence=prominence_val, width=width)
    
    start_idx = len(df) - len(recent_df)
    return peaks + start_idx, troughs + start_idx

def find_double_bottom(df, troughs, tolerance=0.05, min_duration=30, min_retrace=0.1):
    """ì´ì¤‘ ë°”ë‹¥ (Double Bottom) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_troughs = [t for t in troughs if t >= len(df) - 250]
    if len(recent_troughs) < 2: return False, None, None, None
    
    idx2, idx1 = recent_troughs[-1], recent_troughs[-2]
    price1, price2 = df['Close'].iloc[idx1], df['Close'].iloc[idx2]
    
    if idx2 - idx1 < min_duration: return False, None, None, None 
    
    min_price = min(price1, price2)
    max_price = max(price1, price2)
    is_price_matching = (max_price - min_price) / min_price < tolerance
    if not is_price_matching: return False, None, None, None
    
    interim_high = df['Close'].iloc[idx1:idx2].max()
    neckline = interim_high
    
    retrace_from_bottom = neckline - min_price
    if retrace_from_bottom / min_price < min_retrace: return False, None, None, None 
    
    current_price = df['Close'].iloc[-1]
    
    is_breakout = current_price > neckline 
    if is_breakout: return True, neckline, 'Breakout', neckline
    
    retrace_ratio = (current_price - min_price) / (neckline - min_price) if neckline > min_price else 0
    is_potential = retrace_ratio > 0.5 and current_price < neckline
    if is_potential: return False, neckline, 'Potential', neckline
    
    return False, neckline, 'None', neckline 

def find_triple_bottom(df, troughs, tolerance=0.05, min_duration_total=75, min_retrace=0.1):
    """ì‚¼ì¤‘ ë°”ë‹¥ (Triple Bottom) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_troughs = [t for t in troughs if t >= len(df) - 250]
    if len(recent_troughs) < 3: return False, None, None, None
    
    idx3, idx2, idx1 = recent_troughs[-1], recent_troughs[-2], recent_troughs[-3]
    price1, price2, price3 = df['Close'].iloc[idx1], df['Close'].iloc[idx2], df['Close'].iloc[idx3]
    
    if idx3 - idx1 < min_duration_total: return False, None, None, None
    
    min_price = min(price1, price2, price3)
    max_price = max(price1, price2, price3)
    is_price_matching = (max_price - min_price) / min_price < tolerance
    if not is_price_matching: return False, None, None, None
    
    high1 = df['Close'].iloc[idx1:idx2].max()
    high2 = df['Close'].iloc[idx2:idx3].max()
    neckline = max(high1, high2)
    
    retrace_from_bottom = neckline - min_price
    if retrace_from_bottom / min_price < min_retrace: return False, None, None, None
    
    current_price = df['Close'].iloc[-1]
    
    is_breakout = current_price > neckline
    if is_breakout: return True, neckline, 'Breakout', neckline
    
    retrace_ratio = (current_price - min_price) / (neckline - min_price) if neckline > min_price else 0
    is_potential = retrace_ratio > 0.5 and current_price < neckline
    if is_potential: return False, neckline, 'Potential', neckline
    
    return False, neckline, 'None', neckline

def find_cup_and_handle(df, peaks, troughs, handle_drop_ratio=0.3):
    """ì»µ ì•¤ í•¸ë“¤ (Cup and Handle) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_peaks = [p for p in peaks if p >= len(df) - 250]
    if len(recent_peaks) < 2: return False, None, None, None
    
    peak_right_idx = recent_peaks[-1]
    peak_right_price = df['Close'].iloc[peak_right_idx]
    
    handle_start_idx = peak_right_idx
    handle_max_drop = peak_right_price * (1 - handle_drop_ratio) 
    current_price = df['Close'].iloc[-1]
    neckline = peak_right_price 
    
    is_handle_forming = (df['Close'].iloc[handle_start_idx:].max() <= peak_right_price) 
    is_handle_forming &= (current_price > handle_max_drop) 
    
    if is_handle_forming and current_price > neckline:
        return True, neckline, 'Breakout', neckline 
    if is_handle_forming and current_price <= neckline:
        return False, neckline, 'Potential', neckline 
        
    return False, neckline, 'None', neckline 


# ==============================
# 6. ê¸°ìˆ ì  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„
# ==============================

def check_ma_conditions(df, periods, analyze_patterns):
    """ì´ë™ í‰ê· ì„  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    results = {}
    ma_cols = {20: 'SMA_20', 50: 'SMA_50', 200: 'SMA_200'}

    if len(df) < 200: analyze_patterns = False

    # 1. ì£¼ê°€ì™€ MA ë¹„êµ
    for p in periods:
        col_name = ma_cols.get(p)
        if col_name and col_name in df.columns and not df.empty:
            results[f"above_ma{p}"] = df['Close'].iloc[-1] > df[col_name].iloc[-1]
        else:
            results[f"above_ma{p}"] = False

    # 2. ê³¨ë“ /ë°ë“œ í¬ë¡œìŠ¤ ê°ì§€ (50ì¼ì„  vs 200ì¼ì„ )
    ma50_col = ma_cols.get(50)
    ma200_col = ma_cols.get(200)

    if ma50_col in df.columns and ma200_col in df.columns and len(df) >= 200:
        ma50_prev, ma50_curr = df[ma50_col].iloc[-2], df[ma50_col].iloc[-1]
        ma200_prev, ma200_curr = df[ma200_col].iloc[-2], df[ma200_col].iloc[-1]

        results["goldencross_50_200_detected"] = (ma50_prev < ma200_prev and ma50_curr > ma200_curr)
        results["deadcross_50_200_detected"] = (ma50_prev > ma200_prev and ma50_curr < ma200_curr)
    else:
        results["goldencross_50_200_detected"] = False
        results["deadcross_50_200_detected"] = False

    # 3. ê¸°ìˆ ì  íŒ¨í„´ ë¶„ì„ 
    if analyze_patterns:
        peaks, troughs = find_peaks_and_troughs(df)
        
        _, _, db_status, db_price = find_double_bottom(df, troughs)
        _, _, tb_status, tb_price = find_triple_bottom(df, troughs)
        _, _, ch_status, ch_price = find_cup_and_handle(df, peaks, troughs)

        results['pattern_double_bottom_status'] = db_status
        results['db_neckline_price'] = db_price

        results['pattern_triple_bottom_status'] = tb_status
        results['tb_neckline_price'] = tb_price

        results['pattern_cup_and_handle_status'] = ch_status
        results['ch_neckline_price'] = ch_price

    # 4. ì‹œì¥ êµ­ë©´ (Market Regime)
    if 'MarketRegime' in df.columns and not df.empty:
        results['market_regime'] = int(df['MarketRegime'].iloc[-1])
    else:
        results['market_regime'] = -1

    return results


# ==============================
# 6.5. DART API ê³µì‹œ ì¡°íšŒ ë¡œì§
# ==============================

def get_dart_corp_code(symbol):
    """
    ì¢…ëª© ì½”ë“œë¥¼ DART ê³µì‹œ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ì—… ì½”ë“œ(corp_code)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    # ì „ì—­ ë”•ì…”ë„ˆë¦¬ì—ì„œ Corp Codeë¥¼ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤. 
    global DART_CORP_CODE_MAP
    return DART_CORP_CODE_MAP.get(symbol, "")


def fetch_dart_contracts(symbol: str, api_key: str) -> list:
    """
    DART APIë¥¼ í˜¸ì¶œí•˜ì—¬ 'ë‹¨ì¼íŒë§¤Â·ê³µê¸‰ê³„ì•½ ì²´ê²°' ê³µì‹œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    ìµœê·¼ 90ì¼ ì´ë‚´ ê³µì‹œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if not requests or not api_key:
        return []
        
    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=90)).strftime("%Y%m%d")
    
    # ìˆ˜ì •ëœ get_dart_corp_code í•¨ìˆ˜ ì‚¬ìš©
    corp_code = get_dart_corp_code(symbol)
    
    # Corp Codeê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ìš”ì²­ì„ ê±´ë„ˆëœë‹ˆë‹¤.
    if not corp_code:
        logging.debug(f"[{symbol}] DART Corp Codeë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê³µì‹œ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    params = {
        'crtfc_key': api_key,
        'corp_code': corp_code,
        'bgn_de': start_date,
        'end_de': end_date,
        'pblntf_ty': 'A',
        'mrkt_se': 'A',
        'page_no': '1',
        'page_count': '100'
    }
    
    try:
        response = requests.get(DART_API_URL, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        if data.get('status') != '000':
            # '020'ì€ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ì˜ë¯¸ë¡œ, ì˜¤ë¥˜ê°€ ì•„ë‹™ë‹ˆë‹¤.
            if data.get('status') != '020': 
                 logging.warning(f"DART API ì˜¤ë¥˜ ({symbol} / {corp_code}): {data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return []
            
        reports = data.get('list', [])
        
        contract_reports = [
            {
                'title': r['report_nm'],
                'date': r['rcept_dt'],
                'link': f"http://dart.fss.or.kr/dsaf001/main.do?rcpNo={r['rcept_no']}"
            }
            for r in reports if DART_SEARCH_TERM in r['report_nm']
        ]
        
        return contract_reports
        
    except requests.exceptions.RequestException as e:
        logging.error(f"DART API ìš”ì²­ ì‹¤íŒ¨ ({symbol}): {e}")
        return []
    except Exception as e:
        logging.error(f"DART ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({symbol}): {e}")
        return []


# ==============================
# 7. ë¶„ì„ ì‹¤í–‰ ë° ìºì‹± ë¡œì§
# ==============================

def analyze_symbol(item, periods, analyze_patterns, pattern_type_filter, dart_api_key): 
    """ë‹¨ì¼ ì¢…ëª©ì„ ë¶„ì„í•˜ê³  í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = item.get("Code") or item.get("code")
    name = item.get("Name") or item.get("name")
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        logging.debug(f"[{code}] ë°ì´í„° íŒŒì¼ ì—†ìŒ.")
        return None

    try:
        df_raw = pd.read_parquet(path)
        if df_raw.index.dtype != 'datetime64[ns]' and 'Date' in df_raw.columns:
            df_raw = df_raw.set_index('Date')
            
        if df_raw.empty or len(df_raw) < 250:
            logging.debug(f"[{code}] ë°ì´í„° ë¶€ì¡± ({len(df_raw)}ì¼).")
            return None

        df_full = calculate_advanced_features(df_raw)
        df_full = add_market_regime_clustering(df_full)
        
        df_analyze = df_full.iloc[-250:].copy() 

        if len(df_analyze) < 200: 
            logging.debug(f"[{code}] ìµœì¢… ë¶„ì„ ë°ì´í„° ë¶€ì¡± ({len(df_analyze)}ì¼).")
            return None

        analysis_results = check_ma_conditions(df_analyze, periods, analyze_patterns)

        # í•„í„°ë§ ë¡œì§ ì ìš©
        is_match = True
        if pattern_type_filter:
            if pattern_type_filter == 'goldencross':
                is_match = analysis_results.get("goldencross_50_200_detected", False)
            elif pattern_type_filter == 'deadcross': 
                is_match = analysis_results.get("deadcross_50_200_detected", False)
            elif pattern_type_filter in ['double_bottom', 'triple_bottom', 'cup_and_handle']:
                status_key = f'pattern_{pattern_type_filter}_status'
                status = analysis_results.get(status_key)
                is_match = status in ['Breakout', 'Potential']
            elif pattern_type_filter.startswith('regime:'):
                if 'market_regime' in analysis_results:
                    try:
                        target_regime = int(pattern_type_filter.split(':')[1])
                        current_regime = analysis_results['market_regime']
                        is_match = (current_regime == target_regime)
                    except ValueError:
                        is_match = False
                else:
                    is_match = False
            elif pattern_type_filter == 'ma':
                is_match = all(analysis_results.get(f"above_ma{p}", False) for p in periods if p in [20, 50, 200]) # 20, 50, 200ë§Œ í™•ì¸
            elif pattern_type_filter == 'all_below_ma':
                is_match = all(
                    (df_analyze['Close'].iloc[-1] < df_analyze.get(f'SMA_{p}', df_analyze.get(f'ma{p}', 0)).iloc[-1])
                    for p in periods if p in [20, 50, 200]
                )
            else:
                is_match = False

        # DART ê³µì‹œ ì •ë³´ ì¡°íšŒ ë° í†µí•©
        dart_contracts = []
        if dart_api_key:
            dart_contracts = fetch_dart_contracts(code, dart_api_key)


        if pattern_type_filter and not is_match: 
            logging.debug(f"[{code}] í•„í„° '{pattern_type_filter}' ë¶ˆì¼ì¹˜.")
            return None

        if analysis_results:
            analysis_clean = {k: v for k, v in analysis_results.items() if v is not None}
            sort_score = analysis_clean.get('market_regime', -1) 
            
            return {
                "ticker": code,
                "name": name,
                "technical_conditions": analysis_clean, 
                "dart_contracts": dart_contracts, 
                "sort_score": sort_score 
            }
        return None
    except Exception as e:
        logging.error(f"[ERROR] {code} {name} ë¶„ì„ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}")
        return None

# â­â­â­ run_analysis í•¨ìˆ˜ í—¤ë” ë° ë³¸ë¬¸ ìˆ˜ì • (ë‹¨ì¼ ì¢…ëª© í•„í„°ë§ ë¡œì§) â­â­â­
def run_analysis(workers, ma_periods_str, analyze_patterns, pattern_type_filter, top_n, dart_api_key, symbol_filter=None): 
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì „ì²´ ì¢…ëª© ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , ì¼ì¼ ìºì‹±ì„ ì ìš©í•©ë‹ˆë‹¤."""
    
    cleanup_old_cache() 
    
    start_time = time.time()
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()]

    today_str = datetime.now().strftime("%Y%m%d")
    cache_filter_key = f"{pattern_type_filter or 'ma_only'}_{'dart' if dart_api_key else 'no_dart'}" 
    cache_key = f"{today_str}_{cache_filter_key.replace(':', '_')}_{top_n}.json" 
    cache_path = CACHE_DIR / cache_key
    
    # ìºì‹œ í™•ì¸ ë° ë¡œë“œ (ë‹¨ì¼ ì¢…ëª© ë¶„ì„ ì‹œì—ëŠ” ìºì‹œ ë¡œë“œë¥¼ ê±´ë„ˆë›°ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if not symbol_filter and cache_path.exists(): # ë‹¨ì¼ ì¢…ëª© ë¶„ì„ì´ ì•„ë‹ ë•Œë§Œ ìºì‹œ ë¡œë“œ ì‹œë„
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            logging.info(f"ìºì‹œ ë¡œë“œ ì„±ê³µ: {cache_key}")
            sys.stdout.write(json.dumps(cached_data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
            sys.stdout.flush()
            sys.exit(0)
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì‹¤íŒ¨: {e}. ì¬ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")

    # ë¶„ì„ ì‹¤í–‰ ì¤€ë¹„
    if pattern_type_filter and pattern_type_filter not in ['ma', 'all_below_ma'] and not pattern_type_filter.startswith('regime:'):
        analyze_patterns = True

    if 50 not in periods: periods.append(50)
    if 200 not in periods: periods.append(200)

    items = load_listing()
    
    # â­â­â­ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ë‹¨ì¼ ì¢…ëª© í•„í„°ë§ ë¡œì§ â­â­â­
    if symbol_filter:
        items = [item for item in items if (item.get("Code") or item.get("code")) == symbol_filter]
        if not items:
            logging.error(f"ì§€ì •ëœ ì¢…ëª© ì½”ë“œ({symbol_filter})ë¥¼ ë¦¬ìŠ¤íŒ…ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            safe_print_json({"error": "SYMBOL_NOT_FOUND", "ticker": symbol_filter}, status_code=1)
            return
    # â­â­â­ ìˆ˜ì • ë¶€ë¶„ ë â­â­â­
    
    initial_item_count = len(items) # í•„í„°ë§ í›„ ì¢…ëª© ìˆ˜
    total_symbols_loaded = len(load_listing()) # ì›ë˜ ë¡œë“œëœ ì „ì²´ ì¢…ëª© ìˆ˜ (ì§„í–‰ë¥  ë¶„ëª¨ë¡œ ì‚¬ìš©)
    
    if initial_item_count == 0:
        safe_print_json({"error": "LISTING_DATA_EMPTY" if not symbol_filter else "SYMBOL_NOT_FOUND"}, status_code=1)
        return

    results = []
    logging.info(f"ë¶„ì„ ì‹œì‘ (ìºì‹œ ë¯¸ìŠ¤): ì´ {initial_item_count} ì¢…ëª©, í•„í„°: {pattern_type_filter or 'None'}")
    processed_count = 0

    # ìŠ¤ë ˆë“œ í’€ì„ ì´ìš©í•œ ë³‘ë ¬ ë¶„ì„
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_item = {
            executor.submit(analyze_symbol, item, periods, analyze_patterns, pattern_type_filter, dart_api_key): item
            for item in items
        }

        for future in as_completed(future_to_item):
            processed_count += 1
            
            # ë‹¨ì¼ ì¢…ëª©ì´ë“  ì „ì²´ ì¢…ëª©ì´ë“ , í˜„ì¬ ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë¥  ê³„ì‚°
            progress_percent = round((processed_count / initial_item_count) * 100, 2) 

            # ì§„í–‰ ìƒí™© JSON ì¶œë ¥
            sys.stdout.write(json.dumps({
                "mode": "progress",
                "total_symbols": initial_item_count, # í•„í„°ë§ëœ ì¢…ëª© ìˆ˜ (1ê°œ ë˜ëŠ” ì „ì²´)
                "processed_symbols": processed_count,
                "progress_percent": progress_percent
            }, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':')) + "\n")
            sys.stdout.flush()

            try:
                r = future.result()
                if r: results.append(r)
            except Exception as e:
                code = future_to_item[future].get("Code") or future_to_item[future].get("code")
                name = future_to_item[future].get("Name") or future_to_item[future].get("name")
                logging.error(f"[ERROR] {code} {name} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    # ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ Nê°œ ì„ íƒ
    results.sort(key=lambda x: x.get('sort_score', -1), reverse=True)
    final_results = results[:top_n] if top_n > 0 else results
    
    for r in final_results:
        r.pop('sort_score', None)

    end_time = time.time()

    data_check = {
        "listing_file_exists": LISTING_FILE.exists(),
        "total_symbols_loaded": total_symbols_loaded, # ì›ë˜ ë¡œë“œëœ ì „ì²´ ì¢…ëª© ìˆ˜
        "symbols_processed": initial_item_count, # ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜ (í•„í„°ë§ ì ìš© í›„)
        "symbols_filtered": len(results),
        "symbols_returned": len(final_results),
        "time_taken_sec": round(end_time - start_time, 2),
    }

    # ìºì‹œ ì €ì¥ (ë‹¨ì¼ ì¢…ëª© ë¶„ì„ì´ ì•„ë‹ ë•Œë§Œ ì €ì¥)
    final_output = {
        "results": final_results,
        "mode": "analyze_result",
        "filter": pattern_type_filter or 'ma_only',
        "data_check": data_check
    }
    
    if not symbol_filter:
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(final_output, f, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':'))
            logging.info(f"ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    logging.info(f"ë¶„ì„ ì™„ë£Œ ë° ê²°ê³¼ ë°˜í™˜. ì´ ì†Œìš” ì‹œê°„: {data_check['time_taken_sec']}ì´ˆ")
    safe_print_json(final_output, status_code=0)
# â­â­â­ run_analysis í•¨ìˆ˜ ìˆ˜ì • ë â­â­â­


# ==============================
# 8. ì°¨íŠ¸ ìƒì„± ë¡œì§
# ==============================

def generate_chart(symbol, ma_periods_str, chart_period):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ Chart.js JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í¬ë¡œìŠ¤ ì§€ì  ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ í¬í•¨)
    """
    code = symbol
    name = get_stock_name(code)
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()] 
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        safe_print_json({"error": f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}"}, status_code=1)
        return

    try:
        df = pd.read_parquet(path)
        
        if df.index.dtype != 'datetime64[ns]' and 'Date' in df.columns:
            df = df.set_index('Date')
            
        if df.empty:
            safe_print_json({"error": "ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}, status_code=1)
            return

        df_full = calculate_advanced_features(df)
        df_for_chart = df_full.iloc[-chart_period:].copy()

        if df_for_chart.empty:
            safe_print_json({"error": "íŠ¹ì§• ê³„ì‚° í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì°¨íŠ¸ ìƒì„± ë¶ˆê°€."}, status_code=1)
            return

        # 1. ìº”ë“¤ìŠ¤í‹± ë°ì´í„° í¬ë§·íŒ… (OHLCV)
        ohlcv_data = []
        for index, row in df_for_chart.iterrows():
            ohlcv_data.append({
                "x": index.strftime('%Y-%m-%d'), 
                "o": row['Open'], "h": row['High'], "l": row['Low'], "c": row['Close'], "v": row['Volume']
            })

        # 2. ì´ë™í‰ê· ì„ (MA) ë°ì´í„° í¬ë§·íŒ…
        ma_data = {}
        for p in periods:
            ma_col_name = f'SMA_{p}'
            if ma_col_name not in df_for_chart.columns:
                 # ì—†ëŠ” MAë¥¼ ë‹¤ì‹œ ê³„ì‚° (Parquetì— ì €ì¥ë˜ì§€ ì•Šì€ ê²½ìš° ëŒ€ë¹„)
                 df_for_chart[ma_col_name] = df_for_chart['Close'].rolling(window=p, min_periods=1).mean() 

            ma_values = []
            for index, row in df_for_chart.iterrows():
                if not pd.isna(row[ma_col_name]):
                    ma_values.append({"x": index.strftime('%Y-%m-%d'), "y": row[ma_col_name]})
            ma_data[f"MA{p}"] = ma_values
        
        # 3. MACD ë°ì´í„° í¬ë§·íŒ…
        macd_data = {"MACD": [], "Signal": [], "Histogram": []}
        for index, row in df_for_chart.iterrows():
            date_str = index.strftime('%Y-%m-%d')
            if not pd.isna(row['MACD']):
                macd_data["MACD"].append({"x": date_str, "y": row['MACD']})
            if not pd.isna(row['MACD_Signal']):
                macd_data["Signal"].append({"x": date_str, "y": row['MACD_Signal']})
            if not pd.isna(row['MACD_Hist']):
                macd_data["Histogram"].append({"x": date_str, "y": row['MACD_Hist']})

        # 4. í¬ë¡œìŠ¤ ì§€ì  ê°ì§€ ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ì¶”ê°€
        cross_data = []
        pattern_data = []
        
        ma50_col = 'SMA_50'
        ma200_col = 'SMA_200'
        
        # 4-1. MA í¬ë¡œìŠ¤ ì§€ì  ê°ì§€
        if ma50_col in df_for_chart.columns and ma200_col in df_for_chart.columns:
            ma_cross = df_for_chart[ma50_col] > df_for_chart[ma200_col]
            cross_points = ma_cross[ma_cross != ma_cross.shift(1)]

            for date, is_above in cross_points.items():
                if date == df_for_chart.index[0]: continue
                prev_above = ma_cross.shift(1).loc[date]
                cross_type = ""
                
                if not prev_above and is_above: cross_type = "GoldenCross"
                elif prev_above and not is_above: cross_type = "DeadCross"
                
                if cross_type:
                    cross_data.append({"x": date.strftime('%Y-%m-%d'), "y": df_for_chart.loc[date, 'Close'], "type": cross_type})

        # 4-2. íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ê°ì§€
        peaks_all, troughs_all = find_peaks_and_troughs(df_full)
        
        _, db_neckline, db_status, _ = find_double_bottom(df_full, troughs_all)
        _, tb_neckline, tb_status, _ = find_triple_bottom(df_full, troughs_all)
        _, ch_neckline, ch_status, _ = find_cup_and_handle(df_full, peaks_all, troughs_all)

        today_date = df_full.index[-1].strftime('%Y-%m-%d')
        chart_min_close = df_for_chart['Close'].min()
        chart_max_close = df_for_chart['Close'].max()

        patterns_to_check = [
            ("DoubleBottom", db_neckline, db_status),
            ("TripleBottom", tb_neckline, tb_status),
            ("CupAndHandle", ch_neckline, ch_status)
        ]

        for p_name, p_neckline, p_status in patterns_to_check:
            # ì°¨íŠ¸ ë²”ìœ„ ë‚´ì— ë„¥ë¼ì¸ì´ ìˆì„ ë•Œë§Œ í‘œì‹œ
            if p_neckline and (chart_min_close * 0.95 < p_neckline < chart_max_close * 1.05):
                pattern_data.append({"x": today_date, "y": p_neckline, "type": p_name, "status": p_status})


        # 5. ìµœì¢… ê²°ê³¼ JSON êµ¬ì„±
        final_output = {
            "ticker": code,
            "name": name,
            "mode": "chart_data",
            "ohlcv_data": ohlcv_data,
            "ma_data": ma_data,
            "macd_data": macd_data,
            "cross_points": cross_data,
            "pattern_points": pattern_data
        }

        safe_print_json(final_output, status_code=0)

    except Exception as e:
        logging.error(f"[ERROR] Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ({code} {name}): {e}\n{traceback.format_exc()}")
        safe_print_json({"error": f"Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}"}, status_code=1)


# â­â­â­ main í•¨ìˆ˜ ìˆ˜ì • (ì¸ì ì „ë‹¬ ë¡œì§) â­â­â­
def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¸ìˆ˜ë¥¼ íŒŒì‹±í•˜ê³  ëª¨ë“œë³„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--mode", type=str, required=True, choices=['analyze', 'chart'], help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 'analyze' ë˜ëŠ” 'chart'")
    parser.add_argument("--workers", type=int, default=os.cpu_count() * 2, help="ë¶„ì„ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜")
    parser.add_argument("--ma_periods", type=str, default="20,50,200", help="ì´ë™ í‰ê· ì„  ê¸°ê°„ ì§€ì • (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 5,20,50)")
    parser.add_argument("--chart_period", type=int, default=250, help="ì°¨íŠ¸ ëª¨ë“œì—ì„œ í‘œì‹œí•  ê±°ë˜ì¼ ìˆ˜ (ê¸°ë³¸ê°’: 250ì¼)")
    
    # --symbol ì¸ìëŠ” analyzeì™€ chart ëª¨ë‘ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë”°ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    parser.add_argument("--symbol", type=str, help="ë¶„ì„ ë˜ëŠ” ì°¨íŠ¸ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¢…ëª© ì½”ë“œ (Ticker)") 
    
    parser.add_argument("--analyze_patterns", action="store_true", help="íŒ¨í„´ ê°ì§€ í™œì„±í™”")
    parser.add_argument("--pattern_type", type=str,
                         choices=['ma', 'all_below_ma', 'double_bottom', 'triple_bottom', 'cup_and_handle', 'goldencross', 'deadcross', 'regime:0', 'regime:1', 'regime:2', 'regime:3'],
                         help="ë¶„ì„ ëª¨ë“œì—ì„œ í•„í„°ë§í•  íŒ¨í„´ ì¢…ë¥˜ (ì˜ˆ: goldencross, regime:0)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¡œê¹… ë ˆë²¨ DEBUG)")
    parser.add_argument("--top_n", type=int, default=10, help="ë¶„ì„ ê²°ê³¼ ì¤‘ ìƒìœ„ Nê°œ ì¢…ëª©ë§Œ ë°˜í™˜ (0 ì´í•˜: ì „ì²´ ë°˜í™˜)")
    parser.add_argument("--dart_api_key", type=str, default="", help="DART API ì„œë¹„ìŠ¤ í‚¤ (ë¶„ì„ ëª¨ë“œì—ì„œ ê³µì‹œ ì •ë³´ í†µí•© ì‹œ ì‚¬ìš©)")

    args = parser.parse_args()
    
    log_level = logging.DEBUG if args.debug else logging.INFO
    setup_env(log_level)

    try:
        if args.mode == 'analyze':
            run_analysis(
                args.workers, 
                args.ma_periods, 
                args.analyze_patterns, 
                args.pattern_type, 
                args.top_n, 
                args.dart_api_key,
                symbol_filter=args.symbol # â­â­â­ run_analysis í•¨ìˆ˜ë¡œ ì¸ì ì „ë‹¬ â­â­â­
            )
        elif args.mode == 'chart':
            if not args.symbol:
                safe_print_json({"error": "Chart ëª¨ë“œëŠ” --symbol ì¸ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."}, status_code=1)
                return
            generate_chart(args.symbol, args.ma_periods, args.chart_period)
            
    except Exception as e:
        logging.critical(f"ìŠ¤í¬ë¦½íŠ¸ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}")
        safe_print_json({"error": "CRITICAL_RUNTIME_ERROR", "details": str(e)}, status_code=1)

if __name__ == "__main__":
    main()
# â­â­â­ main í•¨ìˆ˜ ìˆ˜ì • ë â­â­â­