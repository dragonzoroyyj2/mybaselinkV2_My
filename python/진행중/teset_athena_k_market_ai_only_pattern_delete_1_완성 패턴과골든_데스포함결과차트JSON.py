# -*- coding: utf-8 -*-
"""
ğŸ“˜ stock_analyzer_ultimate.py (v0.9 - ì£¼ì„ ìƒì„¸ ë²„ì „)
--------------------------------------------
âœ… í•œêµ­ ì£¼ì‹ ì‹œì¥ ë°ì´í„° ë¶„ì„ ë° ê¸°ìˆ ì  íŒ¨í„´ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸
   - ê¸°ëŠ¥: ì¢…ëª© ë¶„ì„ í•„í„°ë§ (analyze ëª¨ë“œ), ì°¨íŠ¸ ì‹œê°í™” ë°ì´í„° ìƒì„± (chart ëª¨ë“œ)
   - íŠ¹ì§•: MA í¬ë¡œìŠ¤ ì§€ì , íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ì°¨íŠ¸ ë°ì´í„°ì— í¬í•¨ë¨
"""

import os
import sys
import json
import time
import logging
import argparse
import traceback
import socket
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import io
import base64
import glob 
import pandas as pd
import numpy as np
from scipy.signal import find_peaks
import ta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# ==============================
# 1. ì´ˆê¸° ì•ˆì „ ê²€ì‚¬ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================

def safe_print_json(data, status_code=1):
    """í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ JSONì„ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    try:
        # JSON ì§ë ¬í™” ë° ì¶œë ¥ (ensure_ascii=Falseë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
        sys.stdout.write(json.dumps(data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
    except Exception as e:
        # JSON ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
        sys.stdout.write(json.dumps({"error": "JSON_SERIALIZATION_FAIL", "original_error": str(e)}, ensure_ascii=False) + "\n")
        
    sys.stdout.flush()
    if status_code != 0:
        sys.exit(status_code)

def check_internet_connection(host="8.8.8.8", port=53, timeout=3):
    """ê°„ë‹¨í•œ ì†Œì¼“ ì—°ê²°ì„ í†µí•´ ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        socket.setdefaulttimeout(timeout)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        s.close()
        return True
    except Exception:
        return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì¸í„°ë„· ì—°ê²° í™•ì¸
if not check_internet_connection():
    safe_print_json({"error": "CRITICAL_ERROR", "reason": "ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "mode": "initial_check"})

# ==============================
# 1.5. JSON Custom Encoder ì •ì˜
# ==============================
class CustomJsonEncoder(json.JSONEncoder):
    """NumPy íƒ€ì… ë° Pandas Timestampë¥¼ í‘œì¤€ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    def default(self, obj):
        # NumPy íƒ€ì… ë³€í™˜
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        if isinstance(obj, (np.floating, np.float64, np.float32)):
            if np.isnan(obj): # NaN ê°’ì€ JSONì—ì„œ nullì´ ë˜ë„ë¡ Noneìœ¼ë¡œ ì²˜ë¦¬
                return None
            return float(obj)
        if isinstance(obj, set):
            return list(obj)
        # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€í™˜
        if isinstance(obj, (pd.Timestamp, datetime, np.datetime64)):
            return obj.strftime('%Y-%m-%d')
        return json.JSONEncoder.default(self, obj)


# ==============================
# 2. ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
# ==============================
BASE_DIR = Path(".").resolve()
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data" # ì¢…ëª© ë°ì´í„°(.parquet)ê°€ ì €ì¥ëœ ìœ„ì¹˜
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json" # ì¢…ëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸ íŒŒì¼
CACHE_DIR = BASE_DIR / "cache" 
LOG_FILE = LOG_DIR / "stock_analyzer_ultimate.log"

# ==============================
# 3. í™˜ê²½ ì´ˆê¸°í™” ë° ìœ í‹¸ë¦¬í‹°
# ==============================
def setup_env(log_level=logging.INFO):
    """í™˜ê²½ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•˜ê³  ë¡œê¹…ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8", mode='a'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def load_listing():
    """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ (stock_listing.json)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    default_item = [{"Code": "005930", "Name": "ì‚¼ì„±ì „ì"}]
    if not LISTING_FILE.exists():
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {LISTING_FILE}")
        return default_item
    try:
        with open(LISTING_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return default_item

def get_stock_name(symbol):
    """ì¢…ëª© ì½”ë“œë¡œ ì´ë¦„ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        items = load_listing()
        for item in items:
            code = item.get("Code") or item.get("code")
            if code == symbol: return item.get("Name") or item.get("name")
        return symbol
    except Exception: return symbol

# ==============================
# 4. ê³ ê¸‰ íŠ¹ì§• ê³µí•™ ë° í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§
# ==============================

def calculate_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """ê³ ê¸‰ íŒ¨í„´ ì¸ì‹ì„ ìœ„í•´ ê¸°ìˆ ì  ì§€í‘œë¥¼ íŠ¹ì§•(Feature)ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."""
    # ëª¨ë©˜í…€ ì§€í‘œ
    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()
    df['MACD'] = ta.trend.MACD(close=df['Close'], fillna=False).macd()
    df['MACD_Signal'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_signal()
    df['MACD_Hist'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_diff() 

    # ë³¼ë¦°ì € ë°´ë“œ ë„ˆë¹„ (ë³€ë™ì„± ì§€í‘œ)
    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)
    df['BB_Width'] = bollinger.bollinger_wband()

    # ì´ë™í‰ê· ì„  (MA) - 20, 50, 200ì¼ì„ 
    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()
    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50, fillna=False).sma_indicator()
    df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200, fillna=False).sma_indicator()

    # ë¡œê·¸ ìˆ˜ìµë¥ 
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))

    # 50ì¼ì„ ì´ 200ì¼ì„  ìœ„ì— ìˆëŠ”ì§€ (ì¶”ì„¸ í”Œë˜ê·¸)
    df['TREND_CROSS'] = (df['SMA_50'] > df['SMA_200']).astype(int)

    feature_subset = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'SMA_200', 'Log_Return']
    df_with_features = df.copy().dropna(subset=feature_subset)
    return df_with_features

def add_market_regime_clustering(df_full: pd.DataFrame, n_clusters=4) -> pd.DataFrame:
    """K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì‹œì¥ êµ­ë©´(Market Regime)ì„ ì •ì˜í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤."""
    feature_cols = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'Log_Return'] 
    min_data_length = 200

    if len(df_full) < min_data_length or not all(col in df_full.columns for col in feature_cols):
        df_full['MarketRegime'] = -1 
        return df_full

    data = df_full[feature_cols].copy()

    if data.drop_duplicates().shape[0] < n_clusters:
        df_full['MarketRegime'] = -1 
        return df_full
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data) # ë°ì´í„° ì •ê·œí™”

    try:
        # K-Means í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (4ê°œì˜ êµ­ë©´ìœ¼ë¡œ ë¶„ë¥˜)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
        df_full['MarketRegime'] = kmeans.fit_predict(scaled_data) # êµ­ë©´ ë²ˆí˜¸ (0, 1, 2, 3) í• ë‹¹
    except ValueError as e:
        df_full['MarketRegime'] = -1

    return df_full


# ==============================
# 5. ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§
# ==============================

def find_peaks_and_troughs(df, prominence_ratio=0.005, width=3):
    """ì£¼ìš” ë´‰ìš°ë¦¬(Peaks)ì™€ ê³¨ì§œê¸°(Troughs) ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤ (ìµœê·¼ 250ì¼ ê¸°ì¤€)."""
    recent_df = df.iloc[-250:].copy()
    if recent_df.empty: return np.array([]), np.array([])
    std_dev = recent_df['Close'].std()
    prominence_val = std_dev * prominence_ratio # ë´‰ìš°ë¦¬/ê³¨ì§œê¸° íŒë‹¨ ê¸°ì¤€ ë†’ì´
    
    # find_peaksëŠ” ì–‘ìˆ˜ ê°’ì—ì„œ ë´‰ìš°ë¦¬ë¥¼ ì°¾ìœ¼ë¯€ë¡œ, ê³¨ì§œê¸°ëŠ” ê°€ê²©ì— -1ì„ ê³±í•˜ì—¬ ì°¾ìŒ
    peaks, _ = find_peaks(recent_df['Close'], prominence=prominence_val, width=width)
    troughs, _ = find_peaks(-recent_df['Close'], prominence=prominence_val, width=width)
    
    start_idx = len(df) - len(recent_df)
    # ì¸ë±ìŠ¤ë¥¼ ì „ì²´ ë°ì´í„°í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return peaks + start_idx, troughs + start_idx

def find_double_bottom(df, troughs, tolerance=0.05, min_duration=30, min_retrace=0.1):
    """ì´ì¤‘ ë°”ë‹¥ (Double Bottom) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_troughs = [t for t in troughs if t >= len(df) - 250]
    if len(recent_troughs) < 2: return False, None, None, None
    
    # ìµœê·¼ ë‘ ê°œì˜ ê³¨ì§œê¸° ì¸ë±ìŠ¤
    idx2, idx1 = recent_troughs[-1], recent_troughs[-2]
    price1, price2 = df['Close'].iloc[idx1], df['Close'].iloc[idx2]
    
    if idx2 - idx1 < min_duration: return False, None, None, None # ìµœì†Œ ê¸°ê°„ í™•ì¸
    
    # ê°€ê²© ì¼ì¹˜ì„± í™•ì¸ (tolerance ë²”ìœ„ ë‚´)
    min_price = min(price1, price2)
    max_price = max(price1, price2)
    is_price_matching = (max_price - min_price) / min_price < tolerance
    if not is_price_matching: return False, None, None, None
    
    # ë„¥ë¼ì¸ (ë‘ ê³¨ì§œê¸° ì‚¬ì´ì˜ ìµœê³ ì )
    interim_high = df['Close'].iloc[idx1:idx2].max()
    neckline = interim_high
    
    # ìµœì†Œ ë°˜ë“± (retrace) í™•ì¸
    retrace_from_bottom = neckline - min_price
    if retrace_from_bottom / min_price < min_retrace: return False, None, None, None 
    
    current_price = df['Close'].iloc[-1]
    
    # íŒ¨í„´ ìƒíƒœ íŒë‹¨
    is_breakout = current_price > neckline # ëŒíŒŒ
    if is_breakout: return True, neckline, 'Breakout', neckline
    
    # ì ì¬ì  í˜•ì„± (ë„¥ë¼ì¸ ëŒíŒŒ ì „, ì–´ëŠ ì •ë„ ë°˜ë“±í–ˆì„ ë•Œ)
    retrace_ratio = (current_price - min_price) / (neckline - min_price) if neckline > min_price else 0
    is_potential = retrace_ratio > 0.5 and current_price < neckline
    if is_potential: return False, neckline, 'Potential', neckline
    
    return False, neckline, 'None', neckline # íŒ¨í„´ ë¶ˆì¼ì¹˜ ì‹œì—ë„ ë„¥ë¼ì¸ ê°€ê²©ì€ ë°˜í™˜ (ì°¨íŠ¸ ì‹œê°í™” ìš©ë„)

def find_triple_bottom(df, troughs, tolerance=0.05, min_duration_total=75, min_retrace=0.1):
    """ì‚¼ì¤‘ ë°”ë‹¥ (Triple Bottom) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_troughs = [t for t in troughs if t >= len(df) - 250]
    if len(recent_troughs) < 3: return False, None, None, None
    
    # ìµœê·¼ ì„¸ ê°œì˜ ê³¨ì§œê¸° ì¸ë±ìŠ¤
    idx3, idx2, idx1 = recent_troughs[-1], recent_troughs[-2], recent_troughs[-3]
    price1, price2, price3 = df['Close'].iloc[idx1], df['Close'].iloc[idx2], df['Close'].iloc[idx3]
    
    if idx3 - idx1 < min_duration_total: return False, None, None, None
    
    # ê°€ê²© ì¼ì¹˜ì„± í™•ì¸
    min_price = min(price1, price2, price3)
    max_price = max(price1, price2, price3)
    is_price_matching = (max_price - min_price) / min_price < tolerance
    if not is_price_matching: return False, None, None, None
    
    # ë„¥ë¼ì¸ (ì„¸ ê³¨ì§œê¸° ì‚¬ì´ì˜ ìµœê³ ì  ì¤‘ ìµœëŒ€ê°’)
    high1 = df['Close'].iloc[idx1:idx2].max()
    high2 = df['Close'].iloc[idx2:idx3].max()
    neckline = max(high1, high2)
    
    # ìµœì†Œ ë°˜ë“± í™•ì¸
    retrace_from_bottom = neckline - min_price
    if retrace_from_bottom / min_price < min_retrace: return False, None, None, None
    
    current_price = df['Close'].iloc[-1]
    
    # íŒ¨í„´ ìƒíƒœ íŒë‹¨ (ëŒíŒŒ/ì ì¬ì  í˜•ì„±)
    is_breakout = current_price > neckline
    if is_breakout: return True, neckline, 'Breakout', neckline
    
    retrace_ratio = (current_price - min_price) / (neckline - min_price) if neckline > min_price else 0
    is_potential = retrace_ratio > 0.5 and current_price < neckline
    if is_potential: return False, neckline, 'Potential', neckline
    
    return False, neckline, 'None', neckline # ë„¥ë¼ì¸ ê°€ê²© ë°˜í™˜

def find_cup_and_handle(df, peaks, troughs, handle_drop_ratio=0.3):
    """ì»µ ì•¤ í•¸ë“¤ (Cup and Handle) íŒ¨í„´ì„ ê°ì§€í•˜ê³  ë„¥ë¼ì¸ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    recent_peaks = [p for p in peaks if p >= len(df) - 250]
    if len(recent_peaks) < 2: return False, None, None, None
    
    peak_right_idx = recent_peaks[-1]
    peak_right_price = df['Close'].iloc[peak_right_idx]
    
    handle_start_idx = peak_right_idx
    handle_max_drop = peak_right_price * (1 - handle_drop_ratio) # í•¸ë“¤ ìµœëŒ€ í•˜ë½ í—ˆìš©ì¹˜
    current_price = df['Close'].iloc[-1]
    neckline = peak_right_price # ì»µì˜ ì˜¤ë¥¸ìª½ ë ê°€ê²©ì´ ë„¥ë¼ì¸
    
    # í•¸ë“¤ í˜•ì„± ì¡°ê±´ í™•ì¸
    is_handle_forming = (df['Close'].iloc[handle_start_idx:].max() <= peak_right_price) # í•¸ë“¤ êµ¬ê°„ ìµœê³ ì ì´ ì»µ ì˜¤ë¥¸ìª½ ëë³´ë‹¤ ë‚®ì•„ì•¼ í•¨
    is_handle_forming &= (current_price > handle_max_drop) # í˜„ì¬ ì£¼ê°€ê°€ í•¸ë“¤ ìµœëŒ€ í•˜ë½ì¹˜ë³´ë‹¤ ë†’ì•„ì•¼ í•¨
    
    if is_handle_forming and current_price > neckline:
        return True, neckline, 'Breakout', neckline # ëŒíŒŒ
    if is_handle_forming and current_price <= neckline:
        return False, neckline, 'Potential', neckline # ì ì¬ì  í˜•ì„±
        
    return False, neckline, 'None', neckline # ë„¥ë¼ì¸ ê°€ê²© ë°˜í™˜

# ==============================
# 6. ê¸°ìˆ ì  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„
# ==============================

def check_ma_conditions(df, periods, analyze_patterns):
    """ì´ë™ í‰ê· ì„  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    results = {}
    ma_cols = {20: 'SMA_20', 50: 'SMA_50', 200: 'SMA_200'}

    if len(df) < 200: analyze_patterns = False

    # 1. ì£¼ê°€ì™€ MA ë¹„êµ (ì¢…ê°€ê°€ MA ìœ„ì— ìˆëŠ”ì§€)
    for p in periods:
        col_name = ma_cols.get(p)
        if col_name and col_name in df.columns and not df.empty:
            # ìµœì¢… ì¢…ê°€ê°€ í•´ë‹¹ MA ê°’ë³´ë‹¤ ë†’ì€ì§€ í™•ì¸
            results[f"above_ma{p}"] = df['Close'].iloc[-1] > df[col_name].iloc[-1]
        else:
            results[f"above_ma{p}"] = False

    # 2. ê³¨ë“ /ë°ë“œ í¬ë¡œìŠ¤ ê°ì§€ (50ì¼ì„  vs 200ì¼ì„ )
    ma50_col = ma_cols.get(50)
    ma200_col = ma_cols.get(200)

    if ma50_col in df.columns and ma200_col in df.columns and len(df) >= 200:
        ma50_prev, ma50_curr = df[ma50_col].iloc[-2], df[ma50_col].iloc[-1]
        ma200_prev, ma200_curr = df[ma200_col].iloc[-2], df[ma200_col].iloc[-1]

        # ê³¨ë“  í¬ë¡œìŠ¤: ì–´ì œ 50 < 200 ì´ì—ˆê³  ì˜¤ëŠ˜ 50 > 200 ì¸ ê²½ìš°
        results["goldencross_50_200_detected"] = (ma50_prev < ma200_prev and ma50_curr > ma200_curr)
        # ë°ë“œ í¬ë¡œìŠ¤: ì–´ì œ 50 > 200 ì´ì—ˆê³  ì˜¤ëŠ˜ 50 < 200 ì¸ ê²½ìš°
        results["deadcross_50_200_detected"] = (ma50_prev > ma200_prev and ma50_curr < ma200_curr)
    else:
        results["goldencross_50_200_detected"] = False
        results["deadcross_50_200_detected"] = False

    # 3. ê¸°ìˆ ì  íŒ¨í„´ ë¶„ì„ (pattern_type í•„í„° ì‚¬ìš© ì‹œ í™œì„±í™”)
    if analyze_patterns:
        peaks, troughs = find_peaks_and_troughs(df)
        
        # ê° íŒ¨í„´ ê°ì§€ í•¨ìˆ˜ ì‹¤í–‰ (ìƒíƒœ, ë„¥ë¼ì¸ ê°€ê²© ë°˜í™˜)
        _, _, db_status, db_price = find_double_bottom(df, troughs)
        _, _, tb_status, tb_price = find_triple_bottom(df, troughs)
        _, _, ch_status, ch_price = find_cup_and_handle(df, peaks, troughs)

        results['pattern_double_bottom_status'] = db_status
        results['db_neckline_price'] = db_price

        results['pattern_triple_bottom_status'] = tb_status
        results['tb_neckline_price'] = tb_price

        results['pattern_cup_and_handle_status'] = ch_status
        results['ch_neckline_price'] = ch_price

    # 4. ì‹œì¥ êµ­ë©´ (Market Regime)
    if 'MarketRegime' in df.columns and not df.empty:
        results['market_regime'] = int(df['MarketRegime'].iloc[-1])
    else:
        results['market_regime'] = -1

    return results

def analyze_symbol(item, periods, analyze_patterns, pattern_type_filter):
    """ë‹¨ì¼ ì¢…ëª©ì„ ë¶„ì„í•˜ê³  í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = item.get("Code") or item.get("code")
    name = item.get("Name") or item.get("name")
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        logging.debug(f"[{code}] ë°ì´í„° íŒŒì¼ ì—†ìŒ.")
        return None

    try:
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        df_raw = pd.read_parquet(path)
        if df_raw.index.dtype != 'datetime64[ns]' and 'Date' in df_raw.columns:
            df_raw = df_raw.set_index('Date')
            
        if df_raw.empty or len(df_raw) < 250:
            logging.debug(f"[{code}] ë°ì´í„° ë¶€ì¡± ({len(df_raw)}ì¼).")
            return None

        # íŠ¹ì§• ê³µí•™ ë° ì‹œì¥ êµ­ë©´ í´ëŸ¬ìŠ¤í„°ë§
        df_full = calculate_advanced_features(df_raw)
        df_full = add_market_regime_clustering(df_full)
        
        df_analyze = df_full.iloc[-250:].copy() # ë¶„ì„ì€ ìµœê·¼ 250ì¼ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰

        if len(df_analyze) < 200: 
            logging.debug(f"[{code}] ìµœì¢… ë¶„ì„ ë°ì´í„° ë¶€ì¡± ({len(df_analyze)}ì¼).")
            return None

        # ê¸°ìˆ ì  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
        analysis_results = check_ma_conditions(df_analyze, periods, analyze_patterns)

        # í•„í„°ë§ ë¡œì§ ì ìš©
        is_match = True
        if pattern_type_filter:
            if pattern_type_filter == 'goldencross':
                is_match = analysis_results.get("goldencross_50_200_detected", False)
            elif pattern_type_filter == 'deadcross': # â­ ë°ë“œí¬ë¡œìŠ¤ í•„í„°
                is_match = analysis_results.get("deadcross_50_200_detected", False)
            elif pattern_type_filter in ['double_bottom', 'triple_bottom', 'cup_and_handle']:
                status_key = f'pattern_{pattern_type_filter}_status'
                status = analysis_results.get(status_key)
                is_match = status in ['Breakout', 'Potential'] # ëŒíŒŒ ë˜ëŠ” ì ì¬ì  í˜•ì„±ì¼ ë•Œ ì¼ì¹˜
            elif pattern_type_filter.startswith('regime:'): # â­ ì‹œì¥ êµ­ë©´ í•„í„° (regime:0, regime:1 ë“±)
                if 'market_regime' in analysis_results:
                    try:
                        target_regime = int(pattern_type_filter.split(':')[1])
                        current_regime = analysis_results['market_regime']
                        is_match = (current_regime == target_regime)
                    except ValueError:
                        is_match = False
                else:
                    is_match = False
            elif pattern_type_filter == 'ma': # ì¢…ê°€ê°€ ëª¨ë“  ì§€ì • MA ìœ„ì— ìˆëŠ” í•„í„°
                is_match = all(analysis_results.get(f"above_ma{p}", False) for p in periods)
            elif pattern_type_filter == 'all_below_ma': # ì¢…ê°€ê°€ ëª¨ë“  ì§€ì • MA ì•„ë˜ì— ìˆëŠ” í•„í„°
                is_match = all(
                    (df_analyze['Close'].iloc[-1] < df_analyze.get(f'SMA_{p}', df_analyze.get(f'ma{p}', 0)).iloc[-1])
                    for p in periods if p in [20, 50, 200]
                )
            else:
                is_match = False

        if pattern_type_filter and not is_match: 
            logging.debug(f"[{code}] í•„í„° '{pattern_type_filter}' ë¶ˆì¼ì¹˜.")
            return None # í•„í„° ì¡°ê±´ ë¶ˆë§Œì¡± ì‹œ ê²°ê³¼ ë°˜í™˜ ì•ˆ í•¨

        if analysis_results:
            analysis_clean = {k: v for k, v in analysis_results.items() if v is not None}
            sort_score = analysis_clean.get('market_regime', -1) # ì‹œì¥ êµ­ë©´ìœ¼ë¡œ ì •ë ¬ ì ìˆ˜ ì‚¬ìš©
            
            return {
                "ticker": code,
                "name": name,
                "technical_conditions": analysis_clean, # ëª¨ë“  ë¶„ì„ ê²°ê³¼ í¬í•¨
                "sort_score": sort_score 
            }
        return None
    except Exception as e:
        logging.error(f"[ERROR] {code} {name} ë¶„ì„ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}")
        return None

# ==============================
# 7. ë¶„ì„ ì‹¤í–‰ ë° ìºì‹± ë¡œì§ (ìƒëµ ì—†ëŠ” ë²„ì „)
# ==============================

def cleanup_old_cache():
    """ì˜¤ëŠ˜ ë‚ ì§œ ì´ì „ì˜ ëª¨ë“  JSON ìºì‹œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    today_str = datetime.now().strftime("%Y%m%d")
    logging.info(f"ìºì‹œ ì •ë¦¬ ì‹œì‘ (ì˜¤ëŠ˜ ë‚ ì§œ: {today_str})")
    cache_files = glob.glob(str(CACHE_DIR / "*.json"))
    deleted_count = 0
    for file_path in cache_files:
        path = Path(file_path)
        file_name_prefix = path.name[:8]
        if file_name_prefix.isdigit() and len(file_name_prefix) == 8:
            file_date_str = file_name_prefix
            if file_date_str < today_str:
                try:
                    os.remove(path)
                    deleted_count += 1
                except Exception as e:
                    logging.error(f"ìºì‹œ íŒŒì¼ {path.name} ì‚­ì œ ì‹¤íŒ¨: {e}")
    logging.info(f"ìºì‹œ ì •ë¦¬ ì™„ë£Œ. ì´ {deleted_count}ê°œ íŒŒì¼ ì‚­ì œë¨.")

def run_analysis(workers, ma_periods_str, analyze_patterns, pattern_type_filter, top_n):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì „ì²´ ì¢…ëª© ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , ì¼ì¼ ìºì‹±ì„ ì ìš©í•©ë‹ˆë‹¤."""
    
    cleanup_old_cache() 
    
    start_time = time.time()
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()]

    today_str = datetime.now().strftime("%Y%m%d")
    cache_filter = pattern_type_filter or 'ma_only'
    cache_key = f"{today_str}_{cache_filter.replace(':', '_')}_{top_n}.json" 
    cache_path = CACHE_DIR / cache_key
    
    # 2. ìºì‹œ í™•ì¸ ë° ë¡œë“œ: ê°™ì€ ì¡°ê±´ìœ¼ë¡œ ë¶„ì„ëœ ì˜¤ëŠ˜ ë‚ ì§œì˜ ìºì‹œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if cache_path.exists():
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            logging.info(f"ìºì‹œ ë¡œë“œ ì„±ê³µ: {cache_key}")
            # ìºì‹œ ê²°ê³¼ë¥¼ stdoutìœ¼ë¡œ ë°”ë¡œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
            sys.stdout.write(json.dumps(cached_data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
            sys.stdout.flush()
            sys.exit(0)
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì‹¤íŒ¨: {e}. ì¬ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")

    # 3. ìºì‹œ ë¯¸ìŠ¤: ë¶„ì„ ì‹¤í–‰ ì¤€ë¹„
    # íŒ¨í„´ í•„í„°ê°€ ì‚¬ìš©ë˜ë©´ íŒ¨í„´ ë¶„ì„ì„ ê°•ì œ í™œì„±í™”
    if pattern_type_filter and pattern_type_filter not in ['ma', 'all_below_ma'] and not pattern_type_filter.startswith('regime:'):
        analyze_patterns = True

    # 50ì¼ì„ , 200ì¼ì„ ì€ í¬ë¡œìŠ¤ ê°ì§€ë¥¼ ìœ„í•´ periodsì— í¬í•¨ë˜ë„ë¡ ì²˜ë¦¬
    if 50 not in periods: periods.append(50)
    if 200 not in periods: periods.append(200)

    items = load_listing()
    initial_item_count = len(items)
    results = []
    logging.info(f"ë¶„ì„ ì‹œì‘ (ìºì‹œ ë¯¸ìŠ¤): ì´ {initial_item_count} ì¢…ëª©, í•„í„°: {pattern_type_filter or 'None'}")
    processed_count = 0

    # ìŠ¤ë ˆë“œ í’€ì„ ì´ìš©í•œ ë³‘ë ¬ ë¶„ì„
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_item = {
            executor.submit(analyze_symbol, item, periods, analyze_patterns, pattern_type_filter): item
            for item in items
        }

        for future in as_completed(future_to_item):
            processed_count += 1
            progress_percent = round((processed_count / initial_item_count) * 100, 2)

            # ì§„í–‰ ìƒí™© JSON ì¶œë ¥
            sys.stdout.write(json.dumps({
                "mode": "progress",
                "total_symbols": initial_item_count,
                "processed_symbols": processed_count,
                "progress_percent": progress_percent
            }, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':')) + "\n")
            sys.stdout.flush()

            try:
                r = future.result()
                if r: results.append(r)
            except Exception as e:
                code = future_to_item[future].get("Code") or future_to_item[future].get("code")
                name = future_to_item[future].get("Name") or future_to_item[future].get("name")
                logging.error(f"[ERROR] {code} {name} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    # 4. ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ Nê°œ ì„ íƒ
    results.sort(key=lambda x: x.get('sort_score', -1), reverse=True)
    final_results = results[:top_n] if top_n > 0 else results
    
    for r in final_results:
        r.pop('sort_score', None) # ìµœì¢… ì¶œë ¥ì—ì„œ ì •ë ¬ ì ìˆ˜ëŠ” ì œê±°

    end_time = time.time()

    data_check = {
        "listing_file_exists": LISTING_FILE.exists(),
        "total_symbols_loaded": initial_item_count,
        "symbols_filtered": len(results),
        "symbols_returned": len(final_results),
        "time_taken_sec": round(end_time - start_time, 2),
    }

    # 5. ìºì‹œ ì €ì¥
    final_output = {
        "results": final_results,
        "mode": "analyze_result",
        "filter": pattern_type_filter or 'ma_only',
        "data_check": data_check
    }
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':'))
        logging.info(f"ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
    except Exception as e:
        logging.error(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    # 6. ìµœì¢… ê²°ê³¼ ì¶œë ¥
    logging.info(f"ë¶„ì„ ì™„ë£Œ ë° ê²°ê³¼ ë°˜í™˜. ì´ ì†Œìš” ì‹œê°„: {data_check['time_taken_sec']}ì´ˆ")
    safe_print_json(final_output, status_code=0)

# ==============================
# 8. ì°¨íŠ¸ ìƒì„± ë¡œì§ (ëª¨ë“  íŒ¨í„´ ì‹œê°í™” í¬ì¸íŠ¸ ì¶”ê°€)
# ==============================

def generate_chart(symbol, ma_periods_str, chart_period):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ Chart.js JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í¬ë¡œìŠ¤ ì§€ì  ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ í¬í•¨)
    """
    code = symbol
    name = get_stock_name(code)
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()] 
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        safe_print_json({"error": f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}"}, status_code=1)
        return

    try:
        df = pd.read_parquet(path)
        
        # ë‚ ì§œ ì¸ë±ìŠ¤ ì„¤ì • í™•ì¸
        if df.index.dtype != 'datetime64[ns]' and 'Date' in df.columns:
            df = df.set_index('Date')
        
        if df.empty:
            safe_print_json({"error": "ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}, status_code=1)
            return

        # íŠ¹ì§• ê³µí•™ í›„ ì°¨íŠ¸ ê¸°ê°„ë§Œí¼ ìŠ¬ë¼ì´ì‹±
        df_full = calculate_advanced_features(df)
        df_for_chart = df_full.iloc[-chart_period:].copy()

        if df_for_chart.empty:
            safe_print_json({"error": "íŠ¹ì§• ê³„ì‚° í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì°¨íŠ¸ ìƒì„± ë¶ˆê°€."}, status_code=1)
            return

        # 1. ìº”ë“¤ìŠ¤í‹± ë°ì´í„° í¬ë§·íŒ… (OHLCV)
        ohlcv_data = []
        for index, row in df_for_chart.iterrows():
            ohlcv_data.append({
                "x": index.strftime('%Y-%m-%d'), 
                "o": row['Open'],
                "h": row['High'],
                "l": row['Low'],
                "c": row['Close'],
                "v": row['Volume']
            })

        # 2. ì´ë™í‰ê· ì„ (MA) ë°ì´í„° í¬ë§·íŒ…
        ma_data = {}
        for p in periods:
            ma_col_name = f'SMA_{p}'
            # MA ê°’ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë‹¤ì‹œ ê³„ì‚° (calculate_advanced_featuresì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
            if ma_col_name not in df_for_chart.columns:
                 df_for_chart[ma_col_name] = df_for_chart['Close'].rolling(window=p, min_periods=1).mean()

            ma_values = []
            for index, row in df_for_chart.iterrows():
                if not pd.isna(row[ma_col_name]):
                    ma_values.append({
                        "x": index.strftime('%Y-%m-%d'),
                        "y": row[ma_col_name]
                    })
            ma_data[f"MA{p}"] = ma_values
        
        # 3. MACD ë°ì´í„° í¬ë§·íŒ…
        macd_data = {
            "MACD": [], 
            "Signal": [], 
            "Histogram": []
        }
        for index, row in df_for_chart.iterrows():
            date_str = index.strftime('%Y-%m-%d')
            if not pd.isna(row['MACD']):
                macd_data["MACD"].append({"x": date_str, "y": row['MACD']})
            if not pd.isna(row['MACD_Signal']):
                macd_data["Signal"].append({"x": date_str, "y": row['MACD_Signal']})
            if not pd.isna(row['MACD_Hist']):
                macd_data["Histogram"].append({"x": date_str, "y": row['MACD_Hist']})

        # â­ 4. í¬ë¡œìŠ¤ ì§€ì  ê°ì§€ ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ì¶”ê°€
        cross_data = []
        pattern_data = []
        
        ma50_col = 'SMA_50'
        ma200_col = 'SMA_200'
        
        # 4-1. MA í¬ë¡œìŠ¤ ì§€ì  ê°ì§€
        if ma50_col in df_for_chart.columns and ma200_col in df_for_chart.columns:
            ma_cross = df_for_chart[ma50_col] > df_for_chart[ma200_col]
            cross_points = ma_cross[ma_cross != ma_cross.shift(1)] # ìƒíƒœê°€ ë°”ë€ ì§€ì  (í¬ë¡œìŠ¤ ë°œìƒ ì§€ì )

            for date, is_above in cross_points.items():
                if date == df_for_chart.index[0]: continue
                prev_above = ma_cross.shift(1).loc[date]
                cross_type = ""
                
                if not prev_above and is_above: cross_type = "GoldenCross" # ì•„ë˜ì—ì„œ ìœ„ë¡œ ëŒíŒŒ
                elif prev_above and not is_above: cross_type = "DeadCross" # ìœ„ì—ì„œ ì•„ë˜ë¡œ ëŒíŒŒ
                
                if cross_type:
                    # í¬ë¡œìŠ¤ ë°œìƒ ë‚ ì§œì™€ ì¢…ê°€ ê¸°ë¡
                    cross_data.append({
                        "x": date.strftime('%Y-%m-%d'),
                        "y": df_for_chart.loc[date, 'Close'],
                        "type": cross_type
                    })

        # 4-2. íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ê°ì§€
        # íŒ¨í„´ ê°ì§€ í•¨ìˆ˜ëŠ” df_full (ì „ì²´ ë°ì´í„°)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        peaks_all, troughs_all = find_peaks_and_troughs(df_full)
        
        # ê° íŒ¨í„´ì˜ ë„¥ë¼ì¸ ê°€ê²©ê³¼ ìƒíƒœë¥¼ ì–»ì–´ì˜´
        _, db_neckline, db_status, _ = find_double_bottom(df_full, troughs_all)
        _, tb_neckline, tb_status, _ = find_triple_bottom(df_full, troughs_all)
        _, ch_neckline, ch_status, _ = find_cup_and_handle(df_full, peaks_all, troughs_all)

        today_date = df_full.index[-1].strftime('%Y-%m-%d')
        chart_min_close = df_for_chart['Close'].min()
        chart_max_close = df_for_chart['Close'].max()

        patterns_to_check = [
            ("DoubleBottom", db_neckline, db_status),
            ("TripleBottom", tb_neckline, tb_status),
            ("CupAndHandle", ch_neckline, ch_status)
        ]

        # ì°¨íŠ¸ ì‹œê°í™”ë¥¼ ìœ„í•´ ë„¥ë¼ì¸ ê°€ê²© ì •ë³´ë¥¼ í¬ë§·íŒ…
        for p_name, p_neckline, p_status in patterns_to_check:
            # ë„¥ë¼ì¸ì´ ìœ íš¨í•˜ê³  ì°¨íŠ¸ ê°€ê²© ë²”ìœ„ ë‚´ì— ìˆì„ ë•Œë§Œ ì¶”ê°€
            if p_neckline and (chart_min_close * 0.95 < p_neckline < chart_max_close * 1.05):
                pattern_data.append({
                    "x": today_date, # ë„¥ë¼ì¸ì€ ê°€ê²© ë ˆë²¨ì´ë¯€ë¡œ, ì°¨íŠ¸ ë ë‚ ì§œì— í‘œì‹œí•  ê°€ê²©ìœ¼ë¡œ ì‚¬ìš©
                    "y": p_neckline,
                    "type": p_name,
                    "status": p_status
                })


        # 5. ìµœì¢… ê²°ê³¼ JSON êµ¬ì„±
        final_output = {
            "ticker": code,
            "name": name,
            "mode": "chart_data",
            "ohlcv_data": ohlcv_data,
            "ma_data": ma_data,
            "macd_data": macd_data,
            "cross_points": cross_data,      # MA í¬ë¡œìŠ¤ ì§€ì  ì •ë³´
            "pattern_points": pattern_data   # íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´
        }

        safe_print_json(final_output, status_code=0)

    except Exception as e:
        logging.error(f"[ERROR] Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ({code} {name}): {e}\n{traceback.format_exc()}")
        safe_print_json({"error": f"Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}"}, status_code=1)

def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¸ìˆ˜ë¥¼ íŒŒì‹±í•˜ê³  ëª¨ë“œë³„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--mode", type=str, required=True, choices=['analyze', 'chart'], help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 'analyze' ë˜ëŠ” 'chart'")
    parser.add_argument("--workers", type=int, default=os.cpu_count() * 2, help="ë¶„ì„ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜")
    parser.add_argument("--ma_periods", type=str, default="20,50,200", help="ì´ë™ í‰ê· ì„  ê¸°ê°„ ì§€ì • (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 5,20,50)")
    parser.add_argument("--chart_period", type=int, default=250, help="ì°¨íŠ¸ ëª¨ë“œì—ì„œ í‘œì‹œí•  ê±°ë˜ì¼ ìˆ˜ (ê¸°ë³¸ê°’: 250ì¼)")
    parser.add_argument("--symbol", type=str, help="ì°¨íŠ¸ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ì¢…ëª© ì½”ë“œ")
    parser.add_argument("--analyze_patterns", action="store_true", help="íŒ¨í„´ ê°ì§€ í™œì„±í™”")
    parser.add_argument("--pattern_type", type=str,
                         choices=['ma', 'all_below_ma', 'double_bottom', 'triple_bottom', 'cup_and_handle', 'goldencross', 'deadcross', 'regime:0', 'regime:1', 'regime:2', 'regime:3'],
                         help="ë¶„ì„ ëª¨ë“œì—ì„œ í•„í„°ë§í•  íŒ¨í„´ ì¢…ë¥˜ (ì˜ˆ: goldencross, regime:0)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¡œê¹… ë ˆë²¨ DEBUG)")
    parser.add_argument("--top_n", type=int, default=10, help="ë¶„ì„ ê²°ê³¼ ì¤‘ ìƒìœ„ Nê°œ ì¢…ëª©ë§Œ ë°˜í™˜ (0 ì´í•˜: ì „ì²´ ë°˜í™˜)")

    args = parser.parse_args()
    
    log_level = logging.DEBUG if args.debug else logging.INFO
    setup_env(log_level)

    try:
        if args.mode == 'analyze':
            run_analysis(args.workers, args.ma_periods, args.analyze_patterns, args.pattern_type, args.top_n)
        elif args.mode == 'chart':
            if not args.symbol:
                safe_print_json({
                    "error": "CRITICAL_ERROR",
                    "reason": "ì°¨íŠ¸ ëª¨ë“œì—ëŠ” --symbol ì¸ìˆ˜ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.",
                    "mode": "argument_check"
                })
            generate_chart(args.symbol, args.ma_periods, args.chart_period)
    except Exception as e:
        error_msg = f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}"
        safe_print_json({
            "error": "CRITICAL_ERROR",
            "reason": error_msg,
            "traceback": traceback.format_exc(),
            "mode": "runtime_error"
        })
    sys.exit(0)

if __file__ != '<stdin>' and __name__ == "__main__":
    main()