# -*- coding: utf-8 -*-
"""
ğŸ“˜ athena_k_market_ai_prod.py (v1.3 - Final Stabilization)
--------------------------------------------
âœ… í•œêµ­ ì£¼ì‹ ì‹œì¥ ë°ì´í„° ë¶„ì„ ë° ê¸°ìˆ ì  íŒ¨í„´ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸
    - ì£¼ìš” ìˆ˜ì •: analyze_symbol() missing 'top_n' ì—ëŸ¬ í•´ê²° ì™„ë£Œ.
    - â­ ì•ˆì •í™”: Long Term Down Trend ë¶„ì„ ì‹œ, MA periods ì¸ìì— ê´€ê³„ì—†ì´
                 SMA_20, SMA_50, SMA_200ì´ í•­ìƒ ì‚¬ìš©ë˜ë„ë¡ ë¡œì§ ë³´ê°•.
"""

import os
import sys
import json
import time
import logging
import argparse
import traceback
import socket
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import glob

import pandas as pd
import numpy as np
import ta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


# ==============================
# 1. ì´ˆê¸° ì•ˆì „ ê²€ì‚¬ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================

def safe_print_json(data, status_code=1):
    """í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ JSONì„ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    try:
        # CustomJsonEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ np íƒ€ì… ë° datetime ê°ì²´ ì²˜ë¦¬
        sys.stdout.write(json.dumps(data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
    except Exception as e:
        sys.stdout.write(json.dumps({"error": "JSON_SERIALIZATION_FAIL", "original_error": str(e)}, ensure_ascii=False) + "\n")
        
    sys.stdout.flush()
    if status_code != 0:
        sys.exit(status_code)

def check_internet_connection(host="8.8.8.8", port=53, timeout=3):
    """ê°„ë‹¨í•œ ì†Œì¼“ ì—°ê²°ì„ í†µí•´ ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        socket.setdefaulttimeout(timeout)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        s.close()
        return True
    except Exception:
        return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì¸í„°ë„· ì—°ê²° í™•ì¸
if not check_internet_connection():
    safe_print_json({"error": "CRITICAL_ERROR", "reason": "ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "mode": "initial_check"})

# ==============================
# 1.5. JSON Custom Encoder ì •ì˜
# ==============================
class CustomJsonEncoder(json.JSONEncoder):
    """NumPy íƒ€ì… ë° Pandas Timestampë¥¼ í‘œì¤€ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        if isinstance(obj, (np.floating, np.float64, np.float32)):
            if np.isnan(obj):
                return None
            return float(obj)
        if isinstance(obj, set):
            return list(obj)
        if isinstance(obj, (pd.Timestamp, datetime, np.datetime64)):
            return obj.strftime('%Y-%m-%d')
        return json.JSONEncoder.default(self, obj)


# ==============================
# 2. ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
# ==============================
# BASE_DIR: ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬
# Path(__file__).resolve().parents[2] ìœ„ì¹˜ëŠ”
#ë¡œì»¬ â†’  ìƒìœ„ 2ë‹¨ê³„ë¡œ ì˜¬ë¼ê°€ë©´ /MyBaseLinkV2/python
#ìš´ì˜ â†’  C:/SET_MyBaseLinkV2/server/python_scripts/python/stock/py
BASE_DIR = Path(__file__).resolve().parents[2]
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data" 
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json" 
CACHE_DIR = BASE_DIR / "cache" 
LOG_FILE = LOG_DIR / "stock_analyzer_ultimate.log"


# ==============================
# 3. í™˜ê²½ ì´ˆê¸°í™” ë° ìœ í‹¸ë¦¬í‹°
# ==============================

def setup_env(log_level=logging.INFO):
    """í™˜ê²½ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•˜ê³  ë¡œê¹…ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8", mode='a'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def load_listing():
    """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ (stock_listing.json)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    default_item = [{"Code": "005930", "Name": "ì‚¼ì„±ì „ì"}]
    if not LISTING_FILE.exists():
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {LISTING_FILE}")
        return default_item
    try:
        with open(LISTING_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return default_item

def get_stock_name(symbol):
    """ì¢…ëª© ì½”ë“œë¡œ ì´ë¦„ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        items = load_listing()
        for item in items:
            code = item.get("Code") or item.get("code")
            if code == symbol: return item.get("Name") or item.get("name")
        return symbol
    except Exception: return symbol

def cleanup_old_cache(days=7):
    """ì§€ì •ëœ ê¸°ê°„(ì¼)ë³´ë‹¤ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    logging.info(f"ë§Œë£Œëœ ({days}ì¼ ì´ìƒ) ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œì‘.")
    
    cutoff_time = datetime.now() - timedelta(days=days)
    
    cache_files = CACHE_DIR.glob('*.json')
    
    deleted_count = 0
    for file_path in cache_files:
        try:
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if mod_time < cutoff_time:
                file_path.unlink()  
                deleted_count += 1
                logging.debug(f"ìºì‹œ íŒŒì¼ ì‚­ì œ: {file_path.name}")
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ {file_path.name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    logging.info(f"ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")


# ==============================
# 4. ê³ ê¸‰ íŠ¹ì§• ê³µí•™ ë° í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§
# ==============================

def calculate_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """ê³ ê¸‰ íŒ¨í„´ ì¸ì‹ì„ ìœ„í•´ ê¸°ìˆ ì  ì§€í‘œë¥¼ íŠ¹ì§•(Feature)ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."""
    
    # í•„ìˆ˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()
    df['MACD'] = ta.trend.MACD(close=df['Close'], fillna=False).macd()
    df['MACD_Signal'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_signal()
    df['MACD_Hist'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_diff() 

    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)
    df['BB_Width'] = bollinger.bollinger_wband()
    
    # â­ ì•ˆì •í™”: Long Term Down Trend ë¶„ì„ì„ ìœ„í•´ í•„ìˆ˜ MA (20, 50, 200)ëŠ” í•­ìƒ ê³„ì‚°
    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()
    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50, fillna=False).sma_indicator()
    df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200, fillna=False).sma_indicator()

    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    df['TREND_CROSS'] = (df['SMA_50'] > df['SMA_200']).astype(int)

    feature_subset = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'SMA_200', 'Log_Return']
    df_with_features = df.copy().dropna(subset=feature_subset)
    return df_with_features

def add_market_regime_clustering(df_full: pd.DataFrame, n_clusters=4) -> pd.DataFrame:
    """K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì‹œì¥ êµ­ë©´(Market Regime)ì„ ì •ì˜í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤."""
    feature_cols = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'Log_Return'] 
    min_data_length = 200

    if len(df_full) < min_data_length or not all(col in df_full.columns for col in feature_cols):
        df_full['MarketRegime'] = -1 
        return df_full

    data = df_full[feature_cols].copy()

    if data.drop_duplicates().shape[0] < n_clusters:
        df_full['MarketRegime'] = -1 
        return df_full
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data) 

    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
        df_full['MarketRegime'] = kmeans.fit_predict(scaled_data) 
    except ValueError as e:
        df_full['MarketRegime'] = -1

    return df_full


# ==============================
# 5. ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ (í•˜ë½ ì¶”ì„¸ ë¶„ì„)
# ==============================

def find_long_term_down_trend(df: pd.DataFrame):
    """
    ì¥ê¸° í•˜ë½ ì¶”ì„¸ (ì—­ë°°ì—´ ë° ì£¼ê°€ í•˜íšŒ) íŒ¨í„´ì„ ê°ì§€í•©ë‹ˆë‹¤.
    (ì£¼ê°€ < MA20 < MA50 < MA200 ì¡°ê±´ í™•ì¸)
    """
    if len(df) < 200: # 200ì¼ MA ê¸°ì¤€
        return False, "NotEnoughData"

    current_close = df['Close'].iloc[-1]
    
    # í•„ìˆ˜ MA ì»¬ëŸ¼ì€ calculate_advanced_featuresì—ì„œ ë³´ì¥ë¨
    try:
        ma20 = df['SMA_20'].iloc[-1]
        ma50 = df['SMA_50'].iloc[-1]
        ma200 = df['SMA_200'].iloc[-1]
    except KeyError:
        return False, "MA_Missing"

    # 1. ì—­ë°°ì—´ ì¡°ê±´ (MA20 < MA50 < MA200) í™•ì¸
    is_inverse_order = (ma20 < ma50) and (ma50 < ma200)
    
    # 2. ì£¼ê°€ í•˜íšŒ ì¡°ê±´ (ì£¼ê°€ < MA20) í™•ì¸
    is_price_below_ma20 = current_close < ma20
    
    # ìµœì¢… í•˜ë½ ì¶”ì„¸ ì¡°ê±´: ì—­ë°°ì—´ + ì£¼ê°€ í•˜íšŒ
    if is_inverse_order and is_price_below_ma20:
        return True, "StrongDownTrend"
        
    # ì£¼ê°€ëŠ” ëª¨ë“  MA ì•„ë˜ì— ìˆì§€ë§Œ ì—­ë°°ì—´ì´ ì™„ì „í•˜ì§€ ì•Šì€ ê²½ìš°
    is_all_below_ma = (current_close < ma20) and (current_close < ma50) and (current_close < ma200)
    if is_all_below_ma:
         return False, "PotentialDownTrend" 

    return False, "None" 


# ==============================
# 6. ê¸°ìˆ ì  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„
# ==============================

def check_ma_conditions(df, periods, analyze_patterns):
    """ì´ë™ í‰ê· ì„  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    results = {}
    
    # MA ì»¬ëŸ¼ì€ ì´ì œ calculate_advanced_featuresì—ì„œ SMA_20, 50, 200ì´ ëª¨ë‘ ê³„ì‚°ë¨ì„ ê°€ì •
    ma_cols = {20: 'SMA_20', 50: 'SMA_50', 200: 'SMA_200'}

    if len(df) < 200: analyze_patterns = False

    # 1. ì£¼ê°€ì™€ MA ë¹„êµ (periodsëŠ” argparseì—ì„œ ë°›ì€ ê¸°ê°„ë§Œ í™•ì¸)
    for p in periods:
        col_name = ma_cols.get(p)
        
        # NOTE: ë§Œì•½ periodsì— 20, 50, 200 ì™¸ì˜ ë‹¤ë¥¸ ê¸°ê°„ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ë™ì ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•¨.
        # í˜„ì¬ëŠ” 20, 50, 200ë§Œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•˜ê³  ì½”ë“œë¥¼ ê°„ì†Œí™”í•¨.
        if col_name and col_name in df.columns and not df.empty:
            results[f"above_ma{p}"] = df['Close'].iloc[-1] > df[col_name].iloc[-1]
        else:
            results[f"above_ma{p}"] = False

    # 2. ê³¨ë“ /ë°ë“œ í¬ë¡œìŠ¤ ê°ì§€ (50ì¼ì„  vs 200ì¼ì„ )
    ma50_col = ma_cols.get(50)
    ma200_col = ma_cols.get(200)

    if ma50_col in df.columns and ma200_col in df.columns and len(df) >= 200:
        ma50_prev, ma50_curr = df[ma50_col].iloc[-2], df[ma50_col].iloc[-1]
        ma200_prev, ma200_curr = df[ma200_col].iloc[-2], df[ma200_col].iloc[-1]

        results["goldencross_50_200_detected"] = (ma50_prev < ma200_prev and ma50_curr > ma200_curr)
        results["deadcross_50_200_detected"] = (ma50_prev > ma200_prev and ma50_curr < ma200_curr)
    else:
        results["goldencross_50_200_detected"] = False
        results["deadcross_50_200_detected"] = False

    # 3. ê¸°ìˆ ì  íŒ¨í„´ ë¶„ì„ (í•˜ë½ ì¶”ì„¸ ë¶„ì„)
    # NOTE: periods ì¸ìë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŒ (find_long_term_down_trendëŠ” ë‚´ë¶€ì ìœ¼ë¡œ 20, 50, 200 ì‚¬ìš©ì„ ê°€ì •)
    is_down_trend, down_trend_status = find_long_term_down_trend(df) 
    results['pattern_long_term_down_trend'] = is_down_trend
    results['down_trend_status'] = down_trend_status

    # ì´ì „ íŒ¨í„´ë“¤ ë¹„í™œì„±í™”
    results['pattern_double_bottom_status'] = 'Disabled'
    results['db_neckline_price'] = None
    results['pattern_triple_bottom_status'] = 'Disabled'
    results['pattern_cup_and_handle_status'] = 'Disabled'
    results['ch_neckline_price'] = None

    # 4. ì‹œì¥ êµ­ë©´ (Market Regime)
    if 'MarketRegime' in df.columns and not df.empty:
        results['market_regime'] = int(df['MarketRegime'].iloc[-1])
    else:
        results['market_regime'] = -1

    return results


# ==============================
# 7. ë¶„ì„ ì‹¤í–‰ ë° ìºì‹± ë¡œì§
# ==============================

def analyze_symbol(item, periods, analyze_patterns, pattern_type_filter, top_n, symbol_filter=None): 
    """ë‹¨ì¼ ì¢…ëª©ì„ ë¶„ì„í•˜ê³  í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = item.get("Code") or item.get("code")
    name = item.get("Name") or item.get("name")
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        logging.debug(f"[{code}] ë°ì´í„° íŒŒì¼ ì—†ìŒ.")
        return None

    try:
        df_raw = pd.read_parquet(path)
        if df_raw.index.dtype != 'datetime64[ns]' and 'Date' in df_raw.columns:
            df_raw = df_raw.set_index('Date')
            
        if df_raw.empty or len(df_raw) < 250:
            logging.debug(f"[{code}] ë°ì´í„° ë¶€ì¡± ({len(df_raw)}ì¼).")
            return None

        df_full = calculate_advanced_features(df_raw)
        df_full = add_market_regime_clustering(df_full)
        
        df_analyze = df_full.iloc[-250:].copy() 

        if len(df_analyze) < 200: 
            logging.debug(f"[{code}] ìµœì¢… ë¶„ì„ ë°ì´í„° ë¶€ì¡± ({len(df_analyze)}ì¼).")
            return None

        analysis_results = check_ma_conditions(df_analyze, periods, analyze_patterns)

        # í•„í„°ë§ ë¡œì§ ì ìš©
        is_match = True
        if pattern_type_filter:
            if pattern_type_filter == 'goldencross':
                is_match = analysis_results.get("goldencross_50_200_detected", False)
            elif pattern_type_filter == 'deadcross': 
                is_match = analysis_results.get("deadcross_50_200_detected", False)
            elif pattern_type_filter == 'long_term_down_trend':
                is_match = analysis_results.get("pattern_long_term_down_trend", False)
            elif pattern_type_filter.startswith('regime:'):
                if 'market_regime' in analysis_results:
                    try:
                        target_regime = int(pattern_type_filter.split(':')[1])
                        current_regime = analysis_results['market_regime']
                        is_match = (current_regime == target_regime)
                    except ValueError:
                        is_match = False
                else:
                    is_match = False
            elif pattern_type_filter == 'ma':
                is_match = all(analysis_results.get(f"above_ma{p}", False) for p in periods if p in [20, 50, 200])
            elif pattern_type_filter == 'all_below_ma':
                is_match = all(
                    (df_analyze['Close'].iloc[-1] < df_analyze.get(f'SMA_{p}', df_analyze.get(f'ma{p}', 0)).iloc[-1])
                    for p in periods if p in [20, 50, 200]
                )
            else:
                is_match = False

        if pattern_type_filter and not is_match: 
            logging.debug(f"[{code}] í•„í„° '{pattern_type_filter}' ë¶ˆì¼ì¹˜.")
            return None

        if analysis_results:
            analysis_clean = {k: v for k, v in analysis_results.items() if v is not None}
            sort_score = analysis_clean.get('market_regime', -1) 
            
            return {
                "ticker": code,
                "name": name,
                "technical_conditions": analysis_clean, 
                "sort_score": sort_score 
            }
        return None
    except Exception as e:
        logging.error(f"[ERROR] {code} {name} ë¶„ì„ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}") 
        return None

def run_analysis(workers, ma_periods_str, analyze_patterns_flag, pattern_type_filter, top_n, symbol_filter=None): 
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì „ì²´ ì¢…ëª© ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , ì¼ì¼ ìºì‹±ì„ ì ìš©í•©ë‹ˆë‹¤."""
    
    cleanup_old_cache() 
    
    start_time = time.time()
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()]

    today_str = datetime.now().strftime("%Y%m%d")
    analyze_patterns = analyze_patterns_flag 
    
    # ìºì‹œ í‚¤ë¥¼ ìˆœìˆ˜ íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”
    cache_filter_key = f"{pattern_type_filter or 'ma_only'}_{'pattern' if analyze_patterns else 'no_pattern'}"
    cache_key = f"{today_str}_{cache_filter_key.replace(':', '_')}_{top_n}.json" 
    cache_path = CACHE_DIR / cache_key
    
    # ìºì‹œ í™•ì¸ ë° ë¡œë“œ (ë‹¨ì¼ ì¢…ëª© ë¶„ì„ì´ ì•„ë‹ ë•Œë§Œ ìºì‹œ ë¡œë“œ ì‹œë„)
    if not symbol_filter and cache_path.exists(): 
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            logging.info(f"ìºì‹œ ë¡œë“œ ì„±ê³µ: {cache_key}")
            sys.stdout.write(json.dumps(cached_data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
            sys.stdout.flush()
            sys.exit(0)
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì‹¤íŒ¨: {e}. ì¬ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")

    # ë¶„ì„ ì‹¤í–‰ ì¤€ë¹„
    # NOTE: argparse periodsì— 20, 50, 200ì´ ì—†ë”ë¼ë„ calculate_advanced_featuresì—ì„œ ê³„ì‚°ë˜ë¯€ë¡œ, 
    # periods ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ check_ma_conditionsì—ì„œ ë¹„êµ ëŒ€ìƒì— í¬í•¨ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    required_periods = [20, 50, 200]
    for p in required_periods:
        if p not in periods: periods.append(p)

    items = load_listing()
    
    # ë‹¨ì¼ ì¢…ëª© í•„í„°ë§ ë¡œì§
    if symbol_filter:
        items = [item for item in items if (item.get("Code") or item.get("code")) == symbol_filter]
        if not items:
            logging.error(f"ì§€ì •ëœ ì¢…ëª© ì½”ë“œ({symbol_filter})ë¥¼ ë¦¬ìŠ¤íŒ…ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            safe_print_json({"error": "SYMBOL_NOT_FOUND", "ticker": symbol_filter}, status_code=1)
            return
    
    initial_item_count = len(items) 
    total_symbols_loaded = len(load_listing()) 
    
    if initial_item_count == 0:
        safe_print_json({"error": "LISTING_DATA_EMPTY" if not symbol_filter else "SYMBOL_NOT_FOUND"}, status_code=1)
        return

    results = []
    logging.info(f"ë¶„ì„ ì‹œì‘ (ìºì‹œ ë¯¸ìŠ¤): ì´ {initial_item_count} ì¢…ëª©, í•„í„°: {pattern_type_filter or 'None'}")
    processed_count = 0

    # ìŠ¤ë ˆë“œ í’€ì„ ì´ìš©í•œ ë³‘ë ¬ ë¶„ì„
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_item = {
            # â­â­â­ E R R O R   F I X E D â­â­â­
            # analyze_symbol í•¨ìˆ˜ í˜¸ì¶œ ì‹œ 'top_n' ì¸ì ì¶”ê°€
            executor.submit(analyze_symbol, item, periods, analyze_patterns, pattern_type_filter, top_n): item
            for item in items
        }

        for future in as_completed(future_to_item):
            processed_count += 1
            
            # ì§„í–‰ ìƒí™© JSON ì¶œë ¥
            progress_percent = round((processed_count / initial_item_count) * 100, 2) 
            sys.stdout.write(json.dumps({
                "mode": "progress",
                "total_symbols": initial_item_count,
                "processed_symbols": processed_count,
                "progress_percent": progress_percent
            }, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':')) + "\n")
            sys.stdout.flush()

            try:
                r = future.result()
                if r: results.append(r)
            except Exception as e:
                code = future_to_item[future].get("Code") or future_to_item[future].get("code")
                name = future_to_item[future].get("Name") or future_to_item[future].get("name")
                logging.error(f"[ERROR] {code} {name} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}") 

    # ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ Nê°œ ì„ íƒ
    results.sort(key=lambda x: x.get('sort_score', -1), reverse=True)
    final_results = results[:top_n] if top_n > 0 else results
    
    for r in final_results:
        r.pop('sort_score', None)

    end_time = time.time()

    data_check = {
        "listing_file_exists": LISTING_FILE.exists(),
        "total_symbols_loaded": total_symbols_loaded,
        "symbols_processed": initial_item_count,
        "symbols_filtered": len(results),
        "symbols_returned": len(final_results),
        "time_taken_sec": round(end_time - start_time, 2),
    }

    # ìºì‹œ ì €ì¥ (ë‹¨ì¼ ì¢…ëª© ë¶„ì„ì´ ì•„ë‹ ë•Œë§Œ ì €ì¥)
    final_output = {
        "results": final_results,
        "mode": "analyze_result",
        "filter": pattern_type_filter or 'ma_only',
        "data_check": data_check
    }
    
    if not symbol_filter:
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(final_output, f, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':'))
            logging.info(f"ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    logging.info(f"ë¶„ì„ ì™„ë£Œ ë° ê²°ê³¼ ë°˜í™˜. ì´ ì†Œìš” ì‹œê°„: {data_check['time_taken_sec']}ì´ˆ")
    safe_print_json(final_output, status_code=0)


# ==============================
# 8. ì°¨íŠ¸ ìƒì„± ë¡œì§
# ==============================

def generate_chart(symbol, ma_periods_str, chart_period):
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ Chart.js JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í¬ë¡œìŠ¤ ì§€ì  ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ í¬í•¨)
    """
    code = symbol
    name = get_stock_name(code)
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()] 
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        safe_print_json({"error": f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}"}, status_code=1)
        return

    try:
        df = pd.read_parquet(path)
        
        if df.index.dtype != 'datetime64[ns]' and 'Date' in df.columns:
            df = df.set_index('Date')
            
        if df.empty:
            safe_print_json({"error": "ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}, status_code=1)
            return

        df_full = calculate_advanced_features(df)
        df_for_chart = df_full.iloc[-chart_period:].copy()

        if df_for_chart.empty:
            safe_print_json({"error": "íŠ¹ì§• ê³„ì‚° í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì°¨íŠ¸ ìƒì„± ë¶ˆê°€."}, status_code=1)
            return

        # 1. ìº”ë“¤ìŠ¤í‹± ë°ì´í„° í¬ë§·íŒ… (OHLCV)
        ohlcv_data = []
        for index, row in df_for_chart.iterrows():
            ohlcv_data.append({
                "x": index.strftime('%Y-%m-%d'), 
                "o": row['Open'], "h": row['High'], "l": row['Low'], "c": row['Close'], "v": row['Volume']
            })

        # 2. ì´ë™í‰ê· ì„ (MA) ë°ì´í„° í¬ë§·íŒ…
        ma_data = {}
        for p in periods:
            ma_col_name = f'SMA_{p}'
            # calculate_advanced_featuresì—ì„œ 20, 50, 200ì€ ì´ë¯¸ ê³„ì‚°ë¨.
            if ma_col_name not in df_for_chart.columns:
                 df_for_chart[ma_col_name] = df_for_chart['Close'].rolling(window=p, min_periods=1).mean() 

            ma_values = []
            for index, row in df_for_chart.iterrows():
                if not pd.isna(row[ma_col_name]):
                    ma_values.append({"x": index.strftime('%Y-%m-%d'), "y": row[ma_col_name]})
            ma_data[f"MA{p}"] = ma_values
        
        # 3. MACD ë°ì´í„° í¬ë§·íŒ…
        macd_data = {"MACD": [], "Signal": [], "Histogram": []}
        for index, row in df_for_chart.iterrows():
            date_str = index.strftime('%Y-%m-%d')
            if not pd.isna(row['MACD']):
                macd_data["MACD"].append({"x": date_str, "y": row['MACD']})
            if not pd.isna(row['MACD_Signal']):
                macd_data["Signal"].append({"x": date_str, "y": row['MACD_Signal']})
            if not pd.isna(row['MACD_Hist']):
                macd_data["Histogram"].append({"x": date_str, "y": row['MACD_Hist']})

        # 4. í¬ë¡œìŠ¤ ì§€ì  ê°ì§€ ë° íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ì¶”ê°€
        cross_data = []
        pattern_data = [] 

        ma50_col = 'SMA_50'
        ma200_col = 'SMA_200'
        
        # 4-1. MA í¬ë¡œìŠ¤ ì§€ì  ê°ì§€
        if ma50_col in df_for_chart.columns and ma200_col in df_for_chart.columns:
            ma_cross = df_for_chart[ma50_col] > df_for_chart[ma200_col]
            cross_points = ma_cross[ma_cross != ma_cross.shift(1)]

            for date, is_above in cross_points.items():
                if date == df_for_chart.index[0]: continue
                prev_above = ma_cross.shift(1).loc[date]
                cross_type = ""
                
                if not prev_above and is_above: cross_type = "GoldenCross"
                elif prev_above and not is_above: cross_type = "DeadCross"
                
                if cross_type:
                    cross_data.append({"x": date.strftime('%Y-%m-%d'), "y": df_for_chart.loc[date, 'Close'], "type": cross_type})

        # 4-2. íŒ¨í„´ ë„¥ë¼ì¸ ì •ë³´ ê°ì§€ (í•˜ë½ ì¶”ì„¸ ìƒíƒœ)
        is_down_trend, down_trend_status = find_long_term_down_trend(df_full) # periods ì¸ì ì œê±°
        today_date = df_full.index[-1].strftime('%Y-%m-%d')

        if is_down_trend:
             # í•˜ë½ ì¶”ì„¸ëŠ” ë„¥ë¼ì¸ ê°€ê²©ì´ ì—†ìœ¼ë¯€ë¡œ, í˜„ì¬ ì¢…ê°€ì™€ ìƒíƒœë§Œ ì „ë‹¬
            pattern_data.append({"x": today_date, "y": df_full['Close'].iloc[-1], "type": "LongTermDownTrend", "status": down_trend_status})
        
        # 5. ìµœì¢… ê²°ê³¼ JSON êµ¬ì„±
        final_output = {
            "ticker": code,
            "name": name,
            "mode": "chart_data",
            "ohlcv_data": ohlcv_data,
            "ma_data": ma_data,
            "macd_data": macd_data,
            "cross_points": cross_data,
            "pattern_points": pattern_data
        }

        safe_print_json(final_output, status_code=0)

    except Exception as e:
        logging.error(f"[ERROR] Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ({code} {name}): {e}\n{traceback.format_exc()}")
        safe_print_json({"error": f"Chart.js ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}"}, status_code=1)


# main í•¨ìˆ˜ ìˆ˜ì • (argparse ì¸ì ëª©ë¡ ë°˜ì˜)
def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¸ìˆ˜ë¥¼ íŒŒì‹±í•˜ê³  ëª¨ë“œë³„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸")
    
    parser.add_argument("--mode", type=str, required=True, choices=['analyze', 'chart'], help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 'analyze' ë˜ëŠ” 'chart'")
    parser.add_argument("--workers", type=int, default=os.cpu_count() * 2, help="ë¶„ì„ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜")
    parser.add_argument("--ma_periods", type=str, default="20,50,200", help="ì´ë™ í‰ê· ì„  ê¸°ê°„ ì§€ì • (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 5,20,50)")
    parser.add_argument("--chart_period", type=int, default=250, help="ì°¨íŠ¸ ëª¨ë“œì—ì„œ í‘œì‹œí•  ê±°ë˜ì¼ ìˆ˜ (ê¸°ë³¸ê°’: 250ì¼)")
    
    parser.add_argument("--symbol", type=str, help="ë¶„ì„ ë˜ëŠ” ì°¨íŠ¸ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¢…ëª© ì½”ë“œ (Ticker)")
    
    parser.add_argument("--analyze_patterns", action="store_true", help="íŒ¨í„´ ê°ì§€ í™œì„±í™” (ì´ì œ í•˜ë½ ì¶”ì„¸ ì™¸ íŒ¨í„´ì€ ë¹„í™œì„±í™” ë¨)")
    parser.add_argument("--pattern_type", type=str,
                          choices=['ma', 'all_below_ma', 'long_term_down_trend', 'goldencross', 'deadcross', 'regime:0', 'regime:1', 'regime:2', 'regime:3'],
                          help="ë¶„ì„ ëª¨ë“œì—ì„œ í•„í„°ë§í•  íŒ¨í„´ ì¢…ë¥˜ (ì˜ˆ: goldencross, long_term_down_trend, regime:0)")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¡œê¹… ë ˆë²¨ DEBUG)")
    parser.add_argument("--top_n", type=int, default=10, help="ë¶„ì„ ê²°ê³¼ ì¤‘ ìƒìœ„ Nê°œ ì¢…ëª©ë§Œ ë°˜í™˜ (0 ì´í•˜: ì „ì²´ ë°˜í™˜)")
    

    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    log_level = logging.DEBUG if args.debug else logging.INFO
    setup_env(log_level=log_level) 
    
    if args.mode == 'analyze':
        analyze_patterns_flag = args.analyze_patterns
        
        run_analysis(
            workers=args.workers,
            ma_periods_str=args.ma_periods,
            analyze_patterns_flag=analyze_patterns_flag, 
            pattern_type_filter=args.pattern_type,
            top_n=args.top_n, 
            symbol_filter=args.symbol 
        )
    elif args.mode == 'chart':
        # chart ëª¨ë“œ ì‹¤í–‰
        if not args.symbol:
            safe_print_json({"error": "MISSING_ARGUMENT", "reason": "ì°¨íŠ¸ ëª¨ë“œëŠ” --symbol ì¸ìë¥¼ í•„ìˆ˜ë¡œ ìš”êµ¬í•©ë‹ˆë‹¤."}, status_code=1)
            return
            
        generate_chart(
            symbol=args.symbol,
            ma_periods_str=args.ma_periods,
            chart_period=args.chart_period
        )

if __name__ == "__main__":
    main()