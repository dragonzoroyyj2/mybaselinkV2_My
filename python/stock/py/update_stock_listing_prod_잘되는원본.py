# -*- coding: utf-8 -*-
"""
ğŸ“˜ update_stock_listing_prod.py (v2.6 ì‹¤ì „ ì•ˆì •íŒ)
----------------------------------------------------------
âœ… StockBatchGProdService(v3.3) ì™„ì „ ë™ê¸°í™”
âœ… BASE_DIR = Path(__file__).resolve().parents[2]
âœ… Spring @Value("${opendart.dart_api_key:}") â†’ --dart_api_key ì¸ì ì™„ì „ ëŒ€ì‘
âœ… DART_API_KEY ì¸ì ì „ë‹¬ ì‹œ í™˜ê²½ë³€ìˆ˜ ìë™ ì„¤ì •
âœ… ë¡œì§/êµ¬ì¡°/ì§„í–‰ë¥ /ë¡œê¹… ê¸°ì¡´ ì™„ì „ ìœ ì§€
----------------------------------------------------------
"""

import os
import sys
import json
import time
import logging
import argparse
import socket
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

try:
    import FinanceDataReader as fdr
    import requests
except ModuleNotFoundError as e:
    print(json.dumps({"error": f"í•„ìˆ˜ ëª¨ë“ˆ ëˆ„ë½: {e.name} ì„¤ì¹˜ í•„ìš”"}, ensure_ascii=False), flush=True)
    sys.exit(1)

# ==============================
# ìƒìˆ˜ ì •ì˜
# ==============================
PER_STOCK_TIMEOUT = 10
MAX_RETRIES = 3
KRX_LIST_CACHE_DAYS = 1

DEFAULT_WORKERS = 16
DEFAULT_HISTORY_YEARS = 3

# ==============================
# ê²½ë¡œ ì„¤ì •
# ==============================
BASE_DIR = Path(__file__).resolve().parents[2]
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data"
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json"
LOG_FILE = LOG_DIR / "robust_stock_updater.log"

# ==============================
# ë¡œê¹… ì´ˆê¸°í™”
# ==============================
def setup_env():
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8"),
            logging.StreamHandler(sys.stdout)
        ]
    )
setup_env()

# ==============================
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
# ==============================
def check_network_connection(host="www.google.com", port=80, timeout=5):
    try:
        socket.setdefaulttimeout(timeout)
        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
        logging.info("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì„±ê³µ.")
        return True
    except Exception as e:
        msg = f"ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨: {e}"
        logging.critical(msg)
        print(json.dumps({"error": msg}, ensure_ascii=False), flush=True)
        sys.exit(1)

# ==============================
# KRX ëª©ë¡ ë¡œë“œ
# ==============================
def load_krx_listing():
    logging.info("[PROGRESS] 5.0 KRX ì¢…ëª© ëª©ë¡ ë¡œë“œ ì¤‘...")
    total = 0
    if LISTING_FILE.exists():
        try:
            krx = pd.read_json(LISTING_FILE, orient="records")
            if not krx.empty:
                total = len(krx)
                logging.info(f"[LOG] KRX ì¢…ëª© ëª©ë¡ ìºì‹œ ë¡œë“œ ({total}ê°œ)")
                logging.info(f"[KRX_TOTAL] {total}")
                logging.info(f"[KRX_SAVED] {total}")
                logging.info("[PROGRESS] 10.0 KRX ëª©ë¡ ë¡œë“œ ì™„ë£Œ (ìºì‹œ)")
                return krx
        except Exception:
            logging.warning("[LOG] KRX ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„")

    try:
        krx = fdr.StockListing("KRX")
        if krx is None or krx.empty:
            raise ValueError("KRX ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        krx.rename(columns={'Symbol': 'Code'}, inplace=True)
        krx["Date"] = datetime.now().strftime("%Y-%m-%d")
        krx.to_json(LISTING_FILE, orient="records", force_ascii=False, indent=2)
        total = len(krx)
        logging.info(f"[LOG] KRX ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ ({total}ê°œ)")
        logging.info(f"[KRX_TOTAL] {total}")
        logging.info(f"[KRX_SAVED] {total}")
        logging.info("[PROGRESS] 10.0 KRX ëª©ë¡ ë¡œë“œ ì™„ë£Œ (ì¬ë‹¤ìš´ë¡œë“œ)")
        return krx
    except Exception as e:
        logging.error(f"[ERROR] KRX ëª©ë¡ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(json.dumps({"status": "failed", "error": str(e)}), flush=True)
        sys.exit(1)

# ==============================
# ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘
# ==============================
def fetch_and_save_data(item: Dict[str, Any], history_years: int, force_download: bool):
    code = item.get("Code")
    name = item.get("Name")
    path = DATA_DIR / f"{code}.parquet"
    end_date = datetime.now().date()
    existing_df = pd.DataFrame()
    last_date = None

    if path.exists() and not force_download:
        try:
            existing_df = pd.read_parquet(path)
            if not existing_df.empty and 'Date' in existing_df.columns:
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                last_date = existing_df['Date'].max().date()
                if last_date >= end_date:
                    return f"{code} {name} â†’ ì´ë¯¸ ìµœì‹ ", "cached"
        except Exception:
            last_date = None

    if last_date and not force_download:
        start_date_str = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')
        update_type = "ì¦ë¶„"
    else:
        start_date_str = (datetime.now() - timedelta(days=history_years * 365)).strftime('%Y-%m-%d')
        update_type = "ì „ì²´"

    for attempt in range(MAX_RETRIES):
        try:
            df = fdr.DataReader(code, start=start_date_str, end=end_date.strftime('%Y-%m-%d'))
            if df.empty:
                return f"{code} {name} â†’ ë°ì´í„° ì—†ìŒ", "no_data"
            df = df.reset_index()
            if update_type == "ì¦ë¶„" and not existing_df.empty:
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                combined_df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates(subset=['Date'], keep='last')
                combined_df.sort_values(by='Date').to_parquet(path, index=False)
                return f"{code} {name} â†’ ì €ì¥ ì™„ë£Œ (ì¦ë¶„, {len(df)}í–‰)", "success"
            else:
                df.to_parquet(path, index=False)
                return f"{code} {name} â†’ ì €ì¥ ì™„ë£Œ ({update_type}, {len(df)}í–‰)", "success"
        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(1 + attempt)
            else:
                return f"{code} {name} â†’ ì‹¤íŒ¨: {type(e).__name__}", "failed"
        except Exception as e:
            return f"{code} {name} â†’ ì‹¤íŒ¨: {type(e).__name__}", "failed"

    return f"{code} {name} â†’ ìµœì¢… ì‹¤íŒ¨", "failed"

# ==============================
# ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
# ==============================
def download_and_save_stocks(krx, workers: int, history_years: int, force_download: bool):
    items = krx.to_dict('records')
    total_count = len(items)
    logging.info(f"[PROGRESS] 25.0 KRX ëª©ë¡ {total_count}ê±´ ë¡œë“œë¨ (ì›Œì»¤: {workers})")
    if force_download:
        logging.info("[LOG] --force ì „ì²´ ë‹¤ìš´ë¡œë“œ ê°•ì œëª¨ë“œ")
    logging.info("[PROGRESS] 30.0 ê°œë³„ ì¢…ëª© ë‹¤ìš´ë¡œë“œ ì‹œì‘")

    update_step = max(1, total_count // 50)
    completed_count = success_count = failed_count = 0

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(fetch_and_save_data, item, history_years, force_download): item for item in items}

        for future in as_completed(futures):
            item = futures[future]
            code = item.get("Code")
            try:
                result_msg, result_type = future.result(timeout=PER_STOCK_TIMEOUT)
                if result_type == "failed":
                    failed_count += 1
                elif result_type != "cached":
                    success_count += 1
                completed_count += 1
                logging.info(f"[LOG] {result_msg} ({completed_count}/{total_count})")
                if (completed_count % update_step == 0) or (completed_count == total_count):
                    pct = 30.0 + (completed_count / total_count) * 70.0
                    logging.info(f"[PROGRESS] {pct:.1f} ì¢…ëª© ì €ì¥ {completed_count}/{total_count}")
            except TimeoutError:
                failed_count += 1
                completed_count += 1
                logging.error(f"[TIMEOUT] {code} â†’ ì‘ë‹µ ì—†ìŒ ({PER_STOCK_TIMEOUT}ì´ˆ ì´ˆê³¼)")
            except Exception as e:
                failed_count += 1
                completed_count += 1
                logging.error(f"[ERROR] {code} ì˜ˆì™¸ ë°œìƒ: {e}")

    progress = 30.0 + (completed_count / total_count) * 70.0 if total_count > 0 else 0.0
    return completed_count, success_count, failed_count, total_count, progress

# ==============================
# ë©”ì¸ ì‹¤í–‰
# ==============================
def main():
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ì‹œì„¸ ë°ì´í„° ì—…ë°ì´íŠ¸ (v2.6)")
    parser.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    parser.add_argument("--history_years", type=int, default=DEFAULT_HISTORY_YEARS)
    parser.add_argument("--force", action="store_true")
    parser.add_argument("--dart_api_key", type=str, default=None)
    args = parser.parse_args()

    # âœ… Springì—ì„œ ë„˜ì–´ì˜¨ DART í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ë°˜ì˜
    if args.dart_api_key:
        os.environ["DART_API_KEY"] = args.dart_api_key
        logging.info(f"[LOG] DART_API_KEY ì¸ì ìˆ˜ì‹  ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ")

    start_time = time.time()
    stats = {"status": "failed", "success": 0, "failed": 0, "total": 0, "progress": 0.0}
    check_network_connection()

    try:
        krx_listing = load_krx_listing()
        completed, success, failed, total, progress = download_and_save_stocks(krx_listing, args.workers, args.history_years, args.force)
        stats.update({"success": success, "failed": failed, "total": total, "progress": round(progress, 1)})

        # âœ… ì¼ë¶€ ì‹¤íŒ¨ëŠ” ì •ìƒ ì¢…ë£Œ
        if completed == total:
            stats["status"] = "completed"
        else:
            stats["status"] = "failed"

    except Exception as e:
        logging.critical(f"[ERROR] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        stats.update({"status": "failed", "error": str(e)})

    finally:
        elapsed = time.time() - start_time
        logging.info(f"[LOG] ì´ ì†Œìš”: {elapsed:.2f}ì´ˆ")

        if stats["status"] == "completed":
            stats["progress"] = 100.0
        logging.info(f"[PROGRESS] {stats['progress']:.1f} ìµœì¢… ì§„í–‰ë¥  ë°˜ì˜")

        print(json.dumps(stats, ensure_ascii=False), flush=True)
        logging.shutdown()

        sys.exit(0 if stats["status"] == "completed" else 1)


if __name__ == "__main__":
    main()
