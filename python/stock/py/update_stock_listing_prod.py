# -*- coding: utf-8 -*-
"""
ğŸ“˜ update_stock_listing_prod_final.py (v2.6 ì‹¤ì „ ì•ˆì •íŒ - ìµœì¢… ê°œì„ íŒ)
----------------------------------------------------------
âœ… StockBatchGProdService(v3.3) ì™„ì „ ë™ê¸°í™”
âœ… BASE_DIR = Path(__file__).resolve().parents[2]
âœ… ë¡œì§/êµ¬ì¡°/ì§„í–‰ë¥ /ë¡œê¹… ê¸°ì¡´ ì™„ì „ ìœ ì§€
----------------------------------------------------------
ğŸŒŸ ê°œì„ ì  ë°˜ì˜ ì™„ë£Œ: KRX ì¢…ëª© ëª©ë¡ ìºì‹œ ê¸°ê°„ (KRX_LIST_CACHE_DAYS) ëª…ì‹œì  ê²€ì‚¬ ë¡œì§ ì ìš©
ğŸ”¥ ê°œì„ ì  ë°˜ì˜ ì™„ë£Œ: fetch_and_save_data ë‚´ ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹… ì ìš©
----------------------------------------------------------
"""

import os
import sys
import json
import time
import logging
import argparse
import socket
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

try:
    import FinanceDataReader as fdr
    import requests
except ModuleNotFoundError as e:
    # í•œêµ­ì–´ ë©”ì‹œì§€ ì¶œë ¥
    print(json.dumps({"error": f"í•„ìˆ˜ ëª¨ë“ˆ ëˆ„ë½: {e.name} ì„¤ì¹˜ í•„ìš”"}, ensure_ascii=False), flush=True)
    sys.exit(1)

# ==============================
# ìƒìˆ˜ ì •ì˜
# ==============================
PER_STOCK_TIMEOUT = 15 # â±ï¸ ì¦ê°€: 10s -> 15s. APIì˜ ëŠë¦° ì‘ë‹µì— ëŒ€ë¹„
MAX_RETRIES = 3
KRX_LIST_CACHE_DAYS = 1 # ğŸŒŸ KRX ëª©ë¡ ìºì‹œ ìœ íš¨ ê¸°ê°„: 1ì¼

DEFAULT_WORKERS = 14	 # ğŸŒŸ ì´ˆì•ˆì •í™”: 8 -> 4ë¡œ ê·¹ë‹¨ì  ê°ì†Œ. ì•ˆì •ì„± ìµœëŒ€í™”
DEFAULT_HISTORY_YEARS = 3

# ==============================
# ê²½ë¡œ ì„¤ì •
# ==============================
# BASE_DIR: ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬
# Path(__file__).resolve().parents[2] ìœ„ì¹˜ëŠ”
#ë¡œì»¬ â†’  ìƒìœ„ 2ë‹¨ê³„ë¡œ ì˜¬ë¼ê°€ë©´ /MyBaseLinkV2/python
#ìš´ì˜ â†’  C:/SET_MyBaseLinkV2/server/python_scripts/python/stock/py
BASE_DIR = Path(__file__).resolve().parents[2]
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data"
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json"
LOG_FILE = LOG_DIR / "robust_stock_updater.log"

# ==============================
# ë¡œê¹… ì´ˆê¸°í™”
# ==============================
def setup_env():
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8"),
            logging.StreamHandler(sys.stdout)
        ]
    )
setup_env()

# ==============================
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
# ==============================
def check_network_connection(host="www.google.com", port=80, timeout=5):
    try:
        socket.setdefaulttimeout(timeout)
        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
        logging.info("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì„±ê³µ.")
        return True
    except Exception as e:
        msg = f"ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨: {e}"
        logging.critical(msg)
        print(json.dumps({"error": msg}, ensure_ascii=False), flush=True)
        sys.exit(1)

# ==============================
# KRX ëª©ë¡ ë¡œë“œ
# ==============================
def load_krx_listing():
    logging.info("[PROGRESS] 5.0 KRX ì¢…ëª© ëª©ë¡ ë¡œë“œ ì¤‘...")
    total = 0
    
    if LISTING_FILE.exists():
        file_mtime = datetime.fromtimestamp(LISTING_FILE.stat().st_mtime).date()
        today = datetime.now().date()
        cache_age = (today - file_mtime).days
        
        # ğŸŒŸ ê°œì„  ë¡œì§: ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ìœ íš¨ ê¸°ê°„(KRX_LIST_CACHE_DAYS) ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        if cache_age < KRX_LIST_CACHE_DAYS:
            try:
                krx = pd.read_json(LISTING_FILE, orient="records")
                if not krx.empty:
                    total = len(krx)
                    logging.info(f"[LOG] KRX ì¢…ëª© ëª©ë¡ ìºì‹œ ë¡œë“œ ({total}ê°œ, ìºì‹œ ê¸°ê°„ {cache_age}ì¼)")
                    logging.info(f"[KRX_TOTAL] {total}")
                    logging.info(f"[KRX_SAVED] {total}")
                    logging.info("[PROGRESS] 10.0 KRX ëª©ë¡ ë¡œë“œ ì™„ë£Œ (ìºì‹œ ìœ íš¨)")
                    return krx
            except Exception:
                logging.warning("[LOG] KRX ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„")
        else:
             logging.warning(f"[LOG] KRX ìºì‹œ ë§Œë£Œ (ê¸°ì¤€ {KRX_LIST_CACHE_DAYS}ì¼, í˜„ì¬ {cache_age}ì¼), ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„")

    try:
        krx = fdr.StockListing("KRX")
        if krx is None or krx.empty:
            raise ValueError("KRX ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        krx.rename(columns={'Symbol': 'Code'}, inplace=True)
        krx["Date"] = datetime.now().strftime("%Y-%m-%d")
        krx.to_json(LISTING_FILE, orient="records", force_ascii=False, indent=2)
        total = len(krx)
        logging.info(f"[LOG] KRX ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ ({total}ê°œ)")
        logging.info(f"[KRX_TOTAL] {total}")
        logging.info(f"[KRX_SAVED] {total}")
        logging.info("[PROGRESS] 10.0 KRX ëª©ë¡ ë¡œë“œ ì™„ë£Œ (ì¬ë‹¤ìš´ë¡œë“œ)")
        return krx
    except Exception as e:
        logging.error(f"[ERROR] KRX ëª©ë¡ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(json.dumps({"status": "failed", "error": str(e)}), flush=True)
        sys.exit(1)

# ==============================
# ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘
# ==============================
def fetch_and_save_data(item: Dict[str, Any], history_years: int, force_download: bool):
    code = item.get("Code")
    name = item.get("Name")
    path = DATA_DIR / f"{code}.parquet"
    end_date = datetime.now().date()
    existing_df = pd.DataFrame()
    last_date = None

    if path.exists() and not force_download:
        try:
            existing_df = pd.read_parquet(path)
            if not existing_df.empty and 'Date' in existing_df.columns:
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                last_date = existing_df['Date'].max().date()
                if last_date >= end_date:
                    return f"{code} {name} â†’ ì´ë¯¸ ìµœì‹ ", "cached"
        except Exception as e:
            # íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ëŠ” ìƒì„¸ ë¡œê·¸ë¥¼ ë‚¨ê¸°ì§€ ì•Šê³ , ì¬ë‹¤ìš´ë¡œë“œ ìœ ë„ (last_date = None)
            logging.warning(f"[{code}] {name} Parquet íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}. ì „ì²´ ì¬ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            last_date = None

    if last_date and not force_download:
        start_date_str = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')
        update_type = "ì¦ë¶„"
    else:
        start_date_str = (datetime.now() - timedelta(days=history_years * 365)).strftime('%Y-%m-%d')
        update_type = "ì „ì²´"

    for attempt in range(MAX_RETRIES):
        try:
            # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ë¥¼ ìœ„í•´ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ë ˆë²¨ì˜ ì˜ˆì™¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨.
            df = fdr.DataReader(code, start=start_date_str, end=end_date.strftime('%Y-%m-%d'))
            if df.empty:
                return f"{code} {name} â†’ ë°ì´í„° ì—†ìŒ", "no_data"
            df = df.reset_index()
            
            if update_type == "ì¦ë¶„" and not existing_df.empty:
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                combined_df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates(subset=['Date'], keep='last')
                combined_df.sort_values(by='Date').to_parquet(path, index=False)
                return f"{code} {name} â†’ ì €ì¥ ì™„ë£Œ (ì¦ë¶„, {len(df)}í–‰)", "success"
            else:
                df.to_parquet(path, index=False)
                return f"{code} {name} â†’ ì €ì¥ ì™„ë£Œ ({update_type}, {len(df)}í–‰)", "success"
        
        except requests.exceptions.RequestException as e:
            # ë„¤íŠ¸ì›Œí¬/ìš”ì²­ ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹… (íƒ€ì„ì•„ì›ƒ í¬í•¨)
            logging.error(f"[{code}] {name} ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜/íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(1 + attempt)
            else:
                return f"{code} {name} â†’ ìµœì¢… ì‹¤íŒ¨: {type(e).__name__}", "failed"
        
        except Exception as e:
            # ğŸ”¥ ì œì‹œëœ ê°œì„ ì‚¬í•­ ë°˜ì˜: ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹… (ê¸°íƒ€ ì˜ˆì™¸) ğŸ”¥
            # exc_info=Falseë¡œ ì„¤ì •í•˜ì—¬ tracebackì€ ë‚¨ê¸°ì§€ ì•Šê³  ë©”ì‹œì§€ë§Œ ìƒì„¸íˆ ê¸°ë¡
            logging.error(f"[{code}] {name} ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ìƒì„¸ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__} - {e}", exc_info=False)
            return f"{code} {name} â†’ ì‹¤íŒ¨: {type(e).__name__}", "failed"

    return f"{code} {name} â†’ ìµœì¢… ì‹¤íŒ¨ (ëª¨ë“  ì¬ì‹œë„ ì†Œì§„)", "failed"

# ==============================
# ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
# ==============================
def download_and_save_stocks(krx, workers: int, history_years: int, force_download: bool):
    items = krx.to_dict('records')
    total_count = len(items)
    logging.info(f"[PROGRESS] 25.0 KRX ëª©ë¡ {total_count}ê±´ ë¡œë“œë¨ (ì›Œì»¤: {workers})")
    if force_download:
        logging.info("[LOG] --force ì „ì²´ ë‹¤ìš´ë¡œë“œ ê°•ì œëª¨ë“œ")
    logging.info("[PROGRESS] 30.0 ê°œë³„ ì¢…ëª© ë‹¤ìš´ë¡œë“œ ì‹œì‘")

    update_step = max(1, total_count // 50)
    completed_count = success_count = failed_count = 0

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(fetch_and_save_data, item, history_years, force_download): item for item in items}

        for future in as_completed(futures):
            item = futures[future]
            code = item.get("Code")
            try:
                # ê°œë³„ ìŠ¤ë ˆë“œ ì‹¤í–‰ ê²°ê³¼ì— ëŒ€í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
                result_msg, result_type = future.result(timeout=PER_STOCK_TIMEOUT + 5) # ìŠ¤ë ˆë“œ ì‹¤í–‰ ìì²´ì— ëŒ€í•œ ì¶”ê°€ íƒ€ì„ì•„ì›ƒ
                
                if result_type == "failed":
                    failed_count += 1
                elif result_type != "cached":
                    success_count += 1
                    
                completed_count += 1
                logging.info(f"[LOG] {result_msg} ({completed_count}/{total_count})")
                
                if (completed_count % update_step == 0) or (completed_count == total_count):
                    pct = 30.0 + (completed_count / total_count) * 70.0
                    logging.info(f"[PROGRESS] {pct:.1f} ì¢…ëª© ì €ì¥ {completed_count}/{total_count}")
                    
            except TimeoutError:
                failed_count += 1
                completed_count += 1
                # TimeoutErrorëŠ” ThreadPoolExecutorì—ì„œ ë°œìƒí•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë¡œê·¸ ê¸°ë¡
                logging.error(f"[TIMEOUT] {code} â†’ ì‘ë‹µ ì—†ìŒ (ì‘ì—… ì‹¤í–‰ {PER_STOCK_TIMEOUT + 5}ì´ˆ ì´ˆê³¼)")
            except Exception as e:
                failed_count += 1
                completed_count += 1
                # ìŠ¤ë ˆë“œ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ê¸°íƒ€ ì¹˜ëª…ì  ì˜¤ë¥˜ ë¡œê¹…
                logging.critical(f"[CRITICAL_ERROR] {code} ì¹˜ëª…ì  ì˜ˆì™¸ ë°œìƒ: {e}")

    progress = 30.0 + (completed_count / total_count) * 70.0 if total_count > 0 else 0.0
    return completed_count, success_count, failed_count, total_count, progress

# ==============================
# ë©”ì¸ ì‹¤í–‰
# ==============================
def main():
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ì‹œì„¸ ë°ì´í„° ì—…ë°ì´íŠ¸ (v2.6)")
    parser.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    parser.add_argument("--history_years", type=int, default=DEFAULT_HISTORY_YEARS)
    parser.add_argument("--force", action="store_true")
    args = parser.parse_args()

    start_time = time.time()
    stats = {"status": "failed", "success": 0, "failed": 0, "total": 0, "progress": 0.0}
    check_network_connection()

    try:
        krx_listing = load_krx_listing()
        completed, success, failed, total, progress = download_and_save_stocks(krx_listing, args.workers, args.history_years, args.force)
        stats.update({"success": success, "failed": failed, "total": total, "progress": round(progress, 1)})

        # âœ… ì¼ë¶€ ì‹¤íŒ¨ëŠ” ì •ìƒ ì¢…ë£Œ (completed == totalì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… status ê²°ì •)
        if completed == total:
            stats["status"] = "completed"
        else:
            # í•˜ë‚˜ë¼ë„ ë¯¸ì™„ë£Œì‹œ failedë¡œ ì²˜ë¦¬ (Timeout í¬í•¨)
            stats["status"] = "failed" 

    except Exception as e:
        logging.critical(f"[ERROR] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        stats.update({"status": "failed", "error": str(e)})

    finally:
        elapsed = time.time() - start_time
        logging.info(f"[LOG] ì´ ì†Œìš”: {elapsed:.2f}ì´ˆ")

        if stats["status"] == "completed":
            stats["progress"] = 100.0
        logging.info(f"[PROGRESS] {stats['progress']:.1f} ìµœì¢… ì§„í–‰ë¥  ë°˜ì˜")

        print(json.dumps(stats, ensure_ascii=False), flush=True)
        logging.shutdown()

        sys.exit(0 if stats["status"] == "completed" else 1)


if __name__ == "__main__":
    main()